<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blackwhole</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-11-28T00:48:41.848Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jin HU</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>cloud computing notes</title>
    <link href="http://yoursite.com/2017/07/28/cloud-computing-notes/"/>
    <id>http://yoursite.com/2017/07/28/cloud-computing-notes/</id>
    <published>2017-07-28T12:38:44.000Z</published>
    <updated>2017-11-28T00:48:41.848Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Cloud-Computing-Chapter1-–-Introduction"><a href="#Cloud-Computing-Chapter1-–-Introduction" class="headerlink" title="Cloud Computing Chapter1 – Introduction"></a>Cloud Computing Chapter1 – Introduction</h1><h2 id="Concepts-of-Cloud-Computing"><a href="#Concepts-of-Cloud-Computing" class="headerlink" title="Concepts of Cloud Computing"></a>Concepts of Cloud Computing</h2><p><strong>✴️ Definitions of Cloud Computing according to NIST ✴️</strong>:  Cloud computing is a model for <em>enabling ubiquitous, convenient , on-demand network access</em> to a shared pool of configurable computing resources that can be <em>rapidly provisioned and released with minimal management effort or service provider interaction</em></p><a id="more"></a><p>Cloud model is composed of:</p><ul><li><p>Five essential characteristics</p><ul><li>On-demand self service: no human interaction required for resource provisioning(配置, supply with)</li><li>​Broad network access: accessible over network</li><li>Resource pooling: pooled resources dynamically shared among consumers, location independence</li><li>Rapid elasticity: Capabilities can be provisioned/released on demand</li><li>measured service: monitored resource usage</li></ul><p>资源配置不用人管，资源被监控、弹性分配和动态共享，可以广泛进入网络</p></li><li><p>🌟Three service models </p><ul><li>Software as a service(SaaS): application runs on cloud infrastructure, consumer access application over the network but doesn’t control/manage underlying infrastructure.</li><li>Platform as a Service(PaaS): Consumer deploy application onto cloud infrastructure using programming languages, libraries, tools supported by provider, no control over underlying infrastructure.</li><li>Infrastructure as a Service(IaaS): Provider provisions processing, storage, network resources to consumer, consumer has no control over underlying infrastructure 💠but can control OS, storage and deployed applications.</li></ul><p>SaaS是只能在云平台上跑应用，PaaS可以用服务商提供的工具部署应用，IaaS是服务商提供了所有基本的配置，三个服务模型都不能控制底层架构，但IaaS可以控制操作系统、存储和部署应用。定制化程度逐渐升高。</p></li><li><p>Four deployment models</p><ul><li>Private Cloud, used by single organization, owned by orgnization or third party or combination, no location premise</li><li>Community cloud, used by organizations with shared concerns, owned by organizations and third party or combination, no location premise</li><li>Public Cloud, open for public, owned by :business:business, 🏫academic, :government, organization and combination, location on premise of cloud provider</li><li>Hybrid Cloud, composition of private/community/public cloud.</li></ul></li></ul><h2 id="Main-Content-of-lecture"><a href="#Main-Content-of-lecture" class="headerlink" title="Main Content of lecture"></a>Main Content of lecture</h2><ol><li>Discussion of IaaS and PaaS: Understanding building blocks, performance characteristics</li><li>Writing scalable and fault-tolerant applications: How to run an application of hundreds of CPUs?</li></ol><p>Goal:</p><ul><li>Understanding the different levels of abstractions</li><li>Understanding the implications of resource sharing</li><li>Learning to take advantage of Cloud platforms</li></ul><h1 id="Cloud-Computing-Chapter-2-–-IaaS"><a href="#Cloud-Computing-Chapter-2-–-IaaS" class="headerlink" title="Cloud Computing Chapter 2 – IaaS"></a>Cloud Computing Chapter 2 – IaaS</h1><p>IaaS – Infrastructure as a Service</p><ul><li>Provider provisions processing, storage, network resources to consumer</li><li>Consumer does not control/manage underlying infrastructure but has control over operating systems, storage, and deployed applications</li></ul><p><em>challenges for IaaS provider</em>:</p><ol><li>Rapid provisioning: consumer can access to resource quickly</li><li>Elasticity: manage data center cost-efficiently, make resources appears infinite</li><li>Isolation: consumers have no interfere with each other</li><li>Performance: maintain good performance.</li></ol><p>对IaaS服务提供商的要求：consumer可以快速访问资源，有效管理资源，consumer间互不干扰，并维持高性能</p><p><em>an approach to deal with these challenges</em> –  <strong>virtualization</strong></p><ul><li>🆗 for rapid provisioning, virtualization can instantiated from pre-complied images, VMs also can be stored as logical volumes on SANs or simply as files. </li><li>🆗 for elasticity, virtualization statistical multiplexing creates illusion of unlimited resources. *customer also has incentive to release idle(闲置) resources</li><li>⛔️ for isolation, what degree of isolation can virtualization really provide? Are resources distributed in a fair manner?</li><li>⛔️ for performance, what is the overhead of virtualization? Can VMs compete with “bare-metal” systems?</li></ul><h2 id="Virtualization"><a href="#Virtualization" class="headerlink" title="Virtualization"></a>Virtualization</h2><p>Virtualization: <em>the simulation of the software and/or hardware upon which other software runs. This simulated environment is called a virtual machine.</em></p><p><em>Real system</em> is often referred as <em>host</em>.</p><p><em>Virtual system</em> is often referred as <em>guest</em>.</p><p>通过虚拟化技术将一台计算机虚拟为多台逻辑计算机</p><h3 id="virtualization-fundamentals"><a href="#virtualization-fundamentals" class="headerlink" title="virtualization fundamentals"></a>virtualization fundamentals</h3><p><strong>Virtualization can be regarded as a <em>translation</em> of host’s state to guest’s state, also operations.</strong></p><p>There is a isomophism $V$, maps guest state to host state. </p><p>virtualization can be classified as <em>Process Virtualization</em> and <em>System Virtualization</em>.</p><p><em>Process Virtualization</em>: application virtual machine, run as a normal application inside a host OS and supports a single process. It is created when that process is started and destroyed when it exits. Its purpose is to provide a platform-independent programming environment that abstracts away details of the underlying hardware or operating system, and allows a program to execute in the same way on any platform. <em>For example Wine software in Linux helps to run Windows application</em> .</p><p><em>System Virtualization</em>: provides a complete system platform which supports the execution of a complete operating system (OS),Just like you said <em>VirtualBox</em> is one example.</p><p><img src="/home/hujin/图片/选区_002.png" alt="选区_002"></p><p>进程虚拟化是指应用创建在主机host中，提供一个不依赖特定平台和底层硬件或os的编程环境，使程序可以在任何平台上跑。比如jvm</p><p>系统虚拟化提供了支持执行完整OS的平台。比如VirtualBox，里面可以开另外一个系统</p><h4 id="Platform-virtualization"><a href="#Platform-virtualization" class="headerlink" title="Platform virtualization"></a>Platform virtualization</h4><ol><li><p>Full Virtualization</p><p>全虚拟化是指虚拟机模拟了完整的底层硬件，包括处理器、物理内存、时钟、外设等，使得为原始硬件设计的操作系统或其它系统软件完全不做任何修改就可以在虚拟机中运行。</p></li><li><p>Paravirtualization</p><p>超虚拟化虚拟机中，部分硬件接口以软件的形式提供给客户机操作系统，这可以通过 Hypercall（VMM 提供给 Guest OS 的直接调用，与系统调用类似）的方式来提供</p></li><li><p>Hardware-Assisted Virtualization</p><p>硬件辅助虚拟化是指借助硬件（主要是主机处理器）的支持来实现高效的全虚拟化。</p></li><li><p>Partial Virtualization</p><p>VMM 只模拟部分底层硬件，因此客户机操作系统不做修改是无法在虚拟机中运行的，其它程序可能也需要进行修改。</p></li><li><p>Operating System Level Virtualization</p><p>操作系统级虚拟化是一种在服务器操作系统中使用的轻量级的虚拟化技术，内核通过创建多个虚拟的操作系统实例（内核和库）来隔离不同的进程，不同实例中的进程完全不了解对方的存在。</p></li></ol><h4 id="Two-basic-designs-for-hardware-virtualization"><a href="#Two-basic-designs-for-hardware-virtualization" class="headerlink" title="Two basic designs for hardware virtualization:"></a>Two basic designs for hardware virtualization:</h4><table><thead><tr><th>VMM type 1</th><th>VMM type 2</th></tr></thead><tbody><tr><td>bare-metal/ hypervisor virtualization</td><td>hosted virtualization</td></tr><tr><td>directly run on hardware</td><td>VMM as host OS process</td></tr><tr><td>basic OS to run schedule VMs</td><td>VMs run as processes supported by VMM</td></tr><tr><td>pro: more efficient</td><td>pro: no special drivers</td></tr><tr><td>con: require special device drivers</td><td>con: more overhead</td></tr></tbody></table><p>Conditions of VMM type I:</p><ul><li><p><strong>VMM must have ultimate control over hardware</strong></p></li><li><p><strong>Guest OS must be disempowered without noticing</strong></p></li><li><p>Four assumptions of Popek and Goldberg:</p><p>One processor and uniformly addressable memory</p><p>Two processor modes: system mode and user mode</p><p>Subset of instruction  set only available in system</p><p>Memory addressing is relative to relocation register</p></li></ul><h4 id="Categories-of-processor-instructions"><a href="#Categories-of-processor-instructions" class="headerlink" title="Categories of processor instructions:"></a>Categories of processor instructions:</h4><ul><li>Privileged instruction can only be executed in system mode</li><li>Sensitive instructions: Control-sensitive change configuration of resource, Behavior-sensitive instructions behave different depending on configuration of resource</li></ul><p><img src="/home/hujin/图片/选区_003.png" alt="选区_003"></p><p>Efficient VMM: all non-sensitive instructions run natively on processor.</p><p>Unprivileged instructions opearted on guest OS(user mode), when Privileged instructions appear, it traps to VMM(system mode), VMM emulates instruction operation, and jump to user target user mode.</p><p>Popek and Goldberg’s requirement are satisfied by IBM Power, Sun Sparc. </p><p>​:lightning: not satisfied by Intel IA-32. But virtualization on IA-32 is possible.  </p><h4 id="Virtualization-of-IA-32"><a href="#Virtualization-of-IA-32" class="headerlink" title="Virtualization of IA-32"></a>Virtualization of IA-32</h4><p>Instructions don’t trap, but have different semantics if not executed in system mode.</p><p>❓ IA-32 doesn’t satisfy Popek and Goldberg’s requirements, how can virtualization on IA-32 is possible? </p><p>IA-32 uses rings to manage privileges.</p><p><img src="/home/hujin/图片/选区_004.png" alt="选区_004"></p><ul><li>Full Virtualization using Binary Translation</li><li>OS-assisted virtualization</li><li>Hardware-assisted virtualization</li></ul><p>VT-x 为 IA 32 处理器增加了两种操作模式：VMX root operation 和 VMX non-root operation。VMM 自己运行在 VMX root operation 模式，VMX non-root operation 模式则由 Guest OS 使用。两种操作模式都支持 Ring 0 ~ Ring 3 这 4 个特权级，因此 VMM 和 Guest OS 都可以自由选择它们所期望的运行级别。</p><p><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-vt/index.html" target="_blank" rel="external">source</a></p><p>virtualization of IA-32 architectures: full, OS-assisted, HW-assisted</p><h3 id="Full-Virtualization"><a href="#Full-Virtualization" class="headerlink" title="Full Virtualization"></a>Full Virtualization</h3><h4 id="Full-virtualization-using-binary-translation"><a href="#Full-virtualization-using-binary-translation" class="headerlink" title="Full virtualization using binary translation"></a>Full virtualization using binary translation</h4><p>translate a book word for word – inefficient</p><p>=&gt; Find critical instructions and replace them</p><ul><li>Run unprevileged instructions on CPU</li><li>Trap and emulate privileged and sensitive instructions</li><li>Find critical instructions and replace with exception</li></ul><p>translation is done lazily. Translation cache is good for frequently used translation units.</p><h5 id="Memory-Management"><a href="#Memory-Management" class="headerlink" title="Memory Management"></a>Memory Management</h5><p>MMU holds a Page Table to look up logical Page and Physical Page, translate logical to physical memory addresses.</p><p>However, page table reside in main memory, overhead of memory access doubles</p><p>每次都要访问主存，查询，取地址，开销很大</p><p>=&gt; Introduce special HW-accelerated cache to remember recent address translations – Translation Lookaside(后备) Buffer(TLB)</p><p>TLB acts as cache of MMU</p><p>​:lightning: TLB is invisible to OS, it is updated by hardware on every page table lookup, must be flushed on every context switch.</p><p>:light: Add another level of indirection</p><p>​:lightning: But additional memory access required to resolve address, inefficient</p><p>​=&gt;:light: Shadow page tables</p><p>guest OS maintain own page tables.</p><p>■ Modifications to guest’s page table trap<br>♦ Entry is copied to the VMM’s shadow page table<br>■ Shadow page table is actually used by hardware<br>♦ Keeps TLB up-to-date<br>♦ Works through virtualization of page table pointer</p><h5 id="Full-Virtualization-and-I-O"><a href="#Full-Virtualization-and-I-O" class="headerlink" title="Full Virtualization and I/O"></a>Full Virtualization and I/O</h5><p>Different levels of I/O virtualization possible:</p><table><thead><tr><th></th><th>content</th><th>pro</th><th>con</th><th>evaluation</th></tr></thead><tbody><tr><td>system call level</td><td>VMM intercepts system call at OS interface</td><td>VMM handles the entire I/O operation</td><td>VMM must shadow OS routines available to the user, Virt. must be transparent to the guest, needs to understand guest OS internals.</td><td>very complicated, hardly seen in practice</td></tr><tr><td>device driver level</td><td>VMM intercepts calls to virtual device driver</td><td>no “reverse engineering” required</td><td>requires knowledge of guest’s device driver interface</td><td>Not generally applicable, OK for many</td></tr><tr><td>I/O operation level</td><td>IA-32 provides special privileged instructions to talk to I/O devices</td><td>all I/O instructions trap</td><td>for low-level instructions, hard for VMM to determine concrete I/O operation, need “reverse engineering”</td><td>difficult for arbitrary devices</td></tr></tbody></table><h5 id="Summery-full-virtualization-with-binary-translation"><a href="#Summery-full-virtualization-with-binary-translation" class="headerlink" title="Summery full virtualization with binary translation"></a>Summery full virtualization with binary translation</h5><p>Don’t require modified guest OS</p><p>Don’t require hardware support</p><p>Good for compute-intensive applications, unprivileged instructions run directly on CPU.</p><p>Degraded performance for data-intensive applications, because I/O requires syscalls which is privileged instructions, “trap and emulate” requires context switches, also the switch will lead to complete flush of TLB</p><h5 id="VMWare-Adaptive-Binary-Translation"><a href="#VMWare-Adaptive-Binary-Translation" class="headerlink" title="VMWare Adaptive Binary Translation"></a>VMWare Adaptive Binary Translation</h5><p>Modern CPUs are deeply pipelined</p><p>Trapping privileged instructions can be too expensive</p><p>VMWare feature: Adaptive Binary Translation</p><ul><li>Monitor frequency and costs of traps</li><li>Adaptively switch between different execution strategies at runtime</li></ul><p>Adaptive Binary Translation improves speed over simpel “trap and emulate”</p><p>​:lightning: But some limitations remain:</p><ul><li>System calls always require VMM intervention(介入)</li><li>Many traps due to shadow table page mechanism</li><li>Instructions for I/O usually trap, context switch for VMM type II required</li></ul><h3 id="OS-assisted-Virtualization-paravirtualization"><a href="#OS-assisted-Virtualization-paravirtualization" class="headerlink" title="OS-assisted Virtualization(paravirtualization)"></a>OS-assisted Virtualization(paravirtualization)</h3><p>Idea:</p><ul><li>make guest OS aware that it is running in a VM</li><li>modify the guest source code so that it avoids assistance of the VMM as far as possible</li></ul><p>Requirements for pure OS-assisted approach</p><ul><li>Source code of guest OS is available</li><li>Modified guest OS maintains application binary interface</li></ul><h4 id="XEN-–-classic-representative-for-paravirtualization"><a href="#XEN-–-classic-representative-for-paravirtualization" class="headerlink" title="XEN  – classic representative for paravirtualization"></a>XEN  – classic representative for paravirtualization</h4><p>XEN architecture and domains:</p><p>Interfaces and Driver concept(XEN 1.0)</p><p>❓ How does XEN tackle full virtualization problems?</p><p>Critical instructions do not trap on IA-32</p><p>​    but guest OS is aware of virtualization</p><p>​    ➡️ so critical instructions can be avoided</p><p>Frequent intervention of the hypervisor required, mostly intervere on page table updates and system call</p><p>➡️ XEN cannot be without interventions either, but XEN plays some tricks to decrease frequency</p><ul><li>Static partitioning among domains</li><li>No guarantee partition is contiguous(邻近的)</li><li>Hypervisor knows which domain “owns” which pages</li></ul><h4 id="XEN-and-memory-virtualization"><a href="#XEN-and-memory-virtualization" class="headerlink" title="XEN and memory virtualization"></a>XEN and memory virtualization</h4><p>XEN lets guests maintain their own page tables which are visible to MMU, if guest OS knows its fraction of physic memory.</p><p>➡️ So no need for hypervisor intervention on read requests. XEN must only validate write requests to ensure isolation.</p><p>Procedure:</p><ol><li>Guest requests page table update via hypercall</li><li>XEN checks if mapping address belongs to domain</li><li>if ok, allows update to page table.</li></ol><p>XEN exists in top 64MB of every logical address space</p><p>​Kernel can access hypervisor without context switch ➡️ no TLB flush</p><p>​General <strong>XEN trick</strong>: command batching 命令批处理, which decreases number of required hypervisor entries/exists. Assume 1024 page table updates, for XEN, Requests collect, not processed immediately, submit with one hypercall ➡️ only one entry/exit required. But for full virtualization, needs 1024 entry/exit.</p><p>Application can call into guest OS without indirection through VMM(ring 0) on each call, because syscalls implemented through software exceptions.</p><ul><li>upon exceptions, HW consults HW exception table to find code to handle exception.</li><li>XEN allows guests to install “fast” exception handler in the hardware exception tale.</li><li>XEN validates handler before install them</li></ul><h4 id="XEN-and-I-O-virtualization"><a href="#XEN-and-I-O-virtualization" class="headerlink" title="XEN and I/O virtualization"></a>XEN and I/O virtualization</h4><p>XEN presents “idealized” hardware abstraction: XEN itself contains specific device drivers, domains only implements lightweight frontend driver.</p><p>I/O data transferred from guest via XEN using shared-memory, async buffer ring.</p><p>In XEN 1.0, device drivers are part of hypervisor: Wrong device drivers can cause system crashes, then virtulization may be affected.</p><p>In XEN 2.0, unmodified device drivers are now loaded in dedicated(专用的) “driver domains”, hypervisor only supervises access to hardware resources and ensure isolation.</p><h4 id="Summary-OS-Assisted-Virtualization"><a href="#Summary-OS-Assisted-Virtualization" class="headerlink" title="**Summary OS-Assisted Virtualization"></a>**Summary OS-Assisted Virtualization</h4><ul><li>​ Required modified guest OS(❓ what is modified guest OS)</li><li>don’t require hadware support</li><li>Pros: better performance through cooperation between hypervisor and guest OS</li><li>Cons: limited compatibility, increased management overhead for data center operator</li></ul><p>Currently, OS-Assisted virtualization is de-facto(事实上的) standard for I/O virtualization. XEN enjoys big support in cloud community.</p><p>All major virtualization solutions provide special drivers.</p><p>HW-assisted virt. became more important.</p><h3 id="Hardware-Assisted-Virtualization"><a href="#Hardware-Assisted-Virtualization" class="headerlink" title="Hardware-Assisted Virtualization"></a>Hardware-Assisted Virtualization</h3><p>Most virtualization difficulties caused by IA-32:</p><ul><li>sensitive instructions do not always trap in rings &gt; 0</li><li>guests can observe they are not running in ring 0</li></ul><p>➡️ extend IA-32 architecture to circumvent virtualization obstacles on the hardware level. (developed by Intel and AMD)</p><p>:surprise: HW support for virtualization keeps increasing</p><h4 id="First-generation-support-VT-x-AMD-V"><a href="#First-generation-support-VT-x-AMD-V" class="headerlink" title="First generation support (VT-x, AMD-V)"></a>First generation support (VT-x, AMD-V)</h4><p>CPU virtualization</p><p>VMM runs in root mode, Guest OS in guest mode. </p><p>VMM and guest run as “co-routines”, VMM can give CPU to guest OS(VM ENTER), also can decide conditions when to regain CPU(VM EXIT).</p><p>VMM controls guest through HW-defined structure, in Intel, it is VMCS, in AMD it is VMCB. (virtual machine control structure/block)</p><p>VMCS/VMCB contains:</p><ul><li>guest state</li><li>control bits defining conditions for VM EXIT</li><li>VMM uses control bits to “confine” and observe guest</li></ul><p>Benefits:</p><ul><li>VMM controls guest through VMCS in fine-grained way: Not all privileged instructions necessarily trap, VMM has flexibility to decide which instructions guest is allowed to handle itself.</li><li>HW extension eliminates many reasons for VMM intervention (like system calls, ring aliasing…)</li></ul><p>​:lightning: But interventions are still needed on several occasions: updates page table, context switches, I/O, interrupts</p><h4 id="Second-Generation-Support-MMU"><a href="#Second-Generation-Support-MMU" class="headerlink" title="Second Generation Support (MMU)"></a>Second Generation Support (MMU)</h4><p>Extended page tables (EPT) / Nested Page Tables(NPT) introduce HW support for memory virtualization.</p><p>Both Intel and AMD introduced tagged TLBs, every TLB entry associated with address space tag, only some entries are invalidated on context switch</p><p>(全虚拟化的TLB只有Logical address和physical address)</p><p>MMU 由 LA -&gt; RA 和RA-&gt;PA的在TLB的映射组成。</p><p>Benefits:</p><ul><li>Significantly less VMM intervention required<ul><li>page table updates, page faults, context switches require no VMM intervention</li></ul></li><li>No shadow page table memory overhead</li><li>Better scalability on multi-core CPUs</li></ul><p>Cost:</p><ul><li>High cost for TLB misses: $O(n^2), n = page\ table\ depth$ </li></ul><h4 id="Third-generation-support"><a href="#Third-generation-support" class="headerlink" title="Third generation support"></a>Third generation support</h4><p>focuses on I/O</p><p>Paravirtualization already decreased CPU overhead for I/O and increased data throughput(吞吐量),  because it have cooperation between virtualized device driver and VMM, and idealized interface reduced VMM interventions.</p><p>​:lightning: However, overhead still too high for high-performance apps</p><p>Goal for HW support:</p><ul><li>High-performance data transfer between device and guest</li><li>Isolation between guests</li></ul><h4 id="summary-HW-Assisted-Virtualization"><a href="#summary-HW-Assisted-Virtualization" class="headerlink" title="summary HW-Assisted Virtualization"></a>summary HW-Assisted Virtualization</h4><p>Not require modified guest OS</p><p>Need HW support</p><p>pros:</p><ul><li>improved performance even for unmodified guest OSs</li><li>good adaption of 1st generation HW-support by VMMs</li><li>2nd generation VMM support increasingly deployed</li></ul><p>cons:</p><ul><li>reduced flexibility due to HW constraints</li></ul><h3 id="Virtual-Machine-Migration"><a href="#Virtual-Machine-Migration" class="headerlink" title="Virtual Machine Migration"></a>Virtual Machine Migration</h3><p>Migration: move VM from one physical host to another</p><p>Why migration: </p><ul><li>Fault management</li><li>Maintenance</li><li>Load balancing</li></ul><p>Desired property:</p><ul><li>no shutdown of the VM</li><li>no distruption of the service</li><li>minimal impact for the user</li></ul><p>Memory migration: ensure <strong>consistency</strong> between source and destination</p><p>Local resources:</p><ul><li>network resources: maintain all open network connections, don’t rely on forwarding of resource host</li><li>storage resources: storage must be accessible both at source and destination</li></ul><h4 id="Strategies-for-Memory-migration"><a href="#Strategies-for-Memory-migration" class="headerlink" title="Strategies for Memory migration"></a>Strategies for Memory migration</h4><ol><li>​Push phase: source VM continues running, sends pages to destination, memory must potentially be sent multiple times ➡️ minimum downtime, potentially long migration time</li><li>​Stop-and-copy phase: Source VM stopped, pages copied to destination, destination is started after received all pages ➡️ short overall migration time, long downtime.</li><li>Pull phase: Execute new VM, pull accessed pages from source.</li></ol><h4 id="strategy-for-migration-in-XEN"><a href="#strategy-for-migration-in-XEN" class="headerlink" title="strategy for migration in XEN"></a>strategy for migration in XEN</h4><ol><li><p>memory migration</p><p>XEN pursues pre-copy strategy for memory migration, combine push and stop-and-copy, balances short downtime with short total migration time</p><p>Iterative approach: multiple rounds of push, short stop-and-copy in the end</p><p>VMWare vMotion use similar approatch.</p><p>Assumption: Source and destination VM are on same IP subnet</p></li><li><p>network migration</p><p>source and destination VM are on same IP subnet.</p><p>Approach:</p><ul><li>Dest. VM have new MAC but old IP address</li><li>After memory transfer, source host sends unsolicited ARP reply: broadcast msg to all hosts on the same network, hosts will remove IP ↔️MAC mapping from caches.</li><li>upon new ARP request, dest. VM will return new MAC address</li></ul></li><li><p>storage migration</p><p>XEN assumes VMs to reside on storage network, migration by rerouting network traffic, similar to network migration.</p></li></ol><h4 id="XEN-migration-timeline"><a href="#XEN-migration-timeline" class="headerlink" title="XEN migration timeline"></a>XEN migration timeline</h4><h3 id="Resource-Fairness-amp-Performance-Implications"><a href="#Resource-Fairness-amp-Performance-Implications" class="headerlink" title="Resource Fairness &amp; Performance Implications"></a>Resource Fairness &amp; Performance Implications</h3><p>In commercial IaaS clouds, many VMs often run on the same physical hardware.</p><p>❓ How is fairness enforced</p><p>Resource Distribution among VMs:</p><ul><li>Storage space: statically partitioned, each VM receives predefined fraction of disk</li><li>Main memory: statically partitioned, each VM receives predefined fraction of RAM</li><li>CPU: use Pinning (each VM is statically assigned CPU cores) or Schedualing (VMM dynamically assigns time slots to VMs)</li><li>I/O access: FCFS</li></ul><p>Since each VM want to receive “fair” share of CPU, to get high CPU utilization and low response time, available algorithms for CPU schedualing are: Borrowed Virtual Time, Atropos, Round RObin, sEDF scheduler, ARINC 653</p><p>​Though currently VM schedualing which focuses on CPU can get good fairness/response times, :lightning: However, processing I/O requests by VMM also consumes CPU time.</p><h3 id="Amazon-Elastic-Compute-Cloud-EC2"><a href="#Amazon-Elastic-Compute-Cloud-EC2" class="headerlink" title="Amazon Elastic Compute Cloud (EC2)"></a>Amazon Elastic Compute Cloud (EC2)</h3><p>Public IaaS cloud by AWS</p><p>Charges fee via per-hour pricing model, customer can shutdown VM at anytime. ➡️ No long-term obligations, no risk of over-/under-provisioning</p><h3 id="Summary-of-IaaS"><a href="#Summary-of-IaaS" class="headerlink" title="Summary of IaaS"></a>Summary of IaaS</h3><p>IaaS clouds let customers rent basic IT resources</p><ul><li>Full control over OS, storage and deployed applications</li><li>no long-term obligation or risk of over/under provisioning</li></ul><p>Virtualization as fundamental enabling technology</p><ul><li>Several customers can share physical infrastructure</li><li>Different approaches to achieve virtulization<ul><li>different levels of abstraction (full, OS-assisted, HW-assisted)</li><li>different performance overhead for different applications</li></ul></li></ul><h1 id="Cloud-computing-chapter-3-PaaS"><a href="#Cloud-computing-chapter-3-PaaS" class="headerlink" title="Cloud computing chapter 3 - PaaS"></a>Cloud computing chapter 3 - PaaS</h1><p>PaaS provide: Programming languages, Libraries, Services, Tools</p><p>Character of services:</p><ul><li>Consumer can deploy custom applications on PaaS cloud using provider’s model</li><li>doesn’t directly control the OS, storage, and deployed apps </li></ul><p>(Unlike Iaas, customization is lower)</p><p>Paas offers higher abstraction level than IaaS, less flexibility and higher provider dependence.</p><p>Charge by time/per query/per msg/CPU usage…</p><p>Consumers can deploy applications have very low operational costs until they become popular</p><p>PaaS value proposition:</p><ul><li>Maintenance: HW, OS, Middleware</li><li>Availability</li><li>Scalability</li></ul><h3 id="Fundamentals-for-scalable-availble-applications"><a href="#Fundamentals-for-scalable-availble-applications" class="headerlink" title="Fundamentals for scalable/availble applications"></a>Fundamentals for scalable/availble applications</h3><h4 id="How-to-Achieve-Scalability"><a href="#How-to-Achieve-Scalability" class="headerlink" title="How to Achieve Scalability?"></a>How to Achieve Scalability?</h4><p>Two principles to scale:</p><p>Multi-tier application scalability entails two questions:</p><ul><li>Where is the bottleneck?</li><li>Is the bottleneck component stateful or stateless?</li></ul><p><strong>Stateless components</strong>: component maintains no internal state beyond a request</p><p><strong>Stateful components</strong>: Component maintains state beyond request required to process next request</p><h5 id="scalability-with-stateless-components"><a href="#scalability-with-stateless-components" class="headerlink" title="scalability with stateless components"></a>scalability with stateless components</h5><p>Approach: <em>More instances of the bottlenect component</em></p><p>Possible levels of Stateless load balancing: Different levels LB can be combined</p><ol><li><p>Load balancing on IP level: Balancing is implemented by IP routers, multiple devices use one IP address, Routers route packet to different locations. Need request fit in one IP packet and control over routers. e.g.: DNS root server.</p></li><li><p>Load Balancing on DNS: implemented by DNS servers, DNS servers resolve DNS name to different IP addresses. Need control over DNS server and stable load characteristics.</p></li><li><p>Load Balancing by distinct load balancer: explicity distributes request among available machines, clients send requests to load balancer. require no network bottleneck.</p></li></ol><p>🔑 Strategies for stateless load balancing:</p><ol><li>Round robin LB: simple, good if all request cause roughtly the same load</li><li>Feedback-based LB: servers report actual load back to load balancer</li><li>Client-based LB: choose server with smallest network latency for client</li></ol><h5 id="Scalability-with-stateful-components"><a href="#Scalability-with-stateful-components" class="headerlink" title="Scalability with stateful components"></a>Scalability with stateful components</h5><p>We assume the data tier is the bottleneck</p><p>database is stateful, store data beyond request, requests from the same client must be handled by the same instance of the db server</p><p>​:light: Idea: Divide data into distinct independent parts, each server is responsible of one or more parts (Partitioning)</p><p>​:lightning: But pure partitioning is not helpful for availability</p><h4 id="Partitioning"><a href="#Partitioning" class="headerlink" title="Partitioning"></a>Partitioning</h4><p>❓ How to partition the data?</p><ul><li><p>Data is now spread across several machines</p></li><li><p>​A task may need data from different places servers together ➡️ causes network traffic</p></li><li><p>More machines add, Scacer the network resource is</p></li><li><p>The goal of every partitioning scheme is to reduce network communication </p><p>😢 however this is highly application-specific </p></li></ul><p>Partition schemes:</p><ol><li><p>Partitioning per tenant</p><p>​Put different tenant on different machines, it’s good for tenants to be isolated and no network traffic between machines. :lightning: But tenent cannot scale beyond one machine</p></li><li><p>Horizontal partitioning (relational databases)</p><p>Split tables by rows, put different rows on different machines, reduced number of rows, reduced indices. e.g.: Google Big Table, MongoDB</p></li><li><p>Vertical partitioning(relational databases)</p><p>split tables by columns, but not very common to improve scalability</p></li></ol><p>How to Distributed Data among Partitions?</p><ul><li>Define key attribute $k \in K$ on the data item to be stored</li><li>Define a globally-known partition function p so that $p: K \rightarrow P$ , $P$ is the set of available partitions</li><li>Characteristics of $p$ will influence load balancing and scability</li></ul><p>Classes of Partition Functions:</p><ul><li>Hash partitioning:<ul><li>Desired property: uniform distribution</li><li>Pro: Good load balancing characteristics</li><li>Con: inefficient for range queries, typically requires data reorganization when number of partitions changes.</li></ul></li><li>Range partitioning:<ul><li>Desired property: If $k_1, k_2 \in K$ are close, $p(k_1), p(k_2) \in P$ shall also be close to each other</li><li>Pro: Efficient for range queries and partition scaling</li><li>Con: Poor load balancing properties.</li></ul></li></ul><h4 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h4><p>​Data is only stored in one machine, if machine goes down, data is gone… ➡️ So we need availability.</p><p><em>Replication: Copies of the data on different machines</em> (where to place? who creates? what if data changes? How to deal with inconsistency?)</p><p><strong>Replication can improve both scalability and availability</strong></p><h5 id="Where-to-place-replicas"><a href="#Where-to-place-replicas" class="headerlink" title="Where to place replicas?"></a>Where to place replicas?</h5><p>In a cloud data center, replica placement depends on network hierarchy. </p><ul><li>one replica on another machine to prevent node failures</li><li>one replica on another rack(层) to prevent outages(service not available or equipment is closed down) of the rack switch</li></ul><p>In global scale, replicas are often distributed with regard to the client locations.</p><ul><li>Chosen to keep network transfers locally confined</li></ul><h5 id="Who-creates-the-copies"><a href="#Who-creates-the-copies" class="headerlink" title="Who creates the copies?"></a>Who creates the copies?</h5><ol><li><p>Server initiated replication</p><p>Copies created by server if popularity of data item increases, in order to reduce server load, server decides among a set of replica servers</p></li><li><p>Client-initiated replication(client caches)</p><p>replica created as result of client’s response, server has no control of cached copy, like web proxies.</p></li></ol><h5 id="What-happens-when-data-changes"><a href="#What-happens-when-data-changes" class="headerlink" title="What happens when data changes?"></a>What happens when data changes?</h5><ol><li>Invalidation protocols: If there are many updates and few reads, when data changed, inform replica servers that their replica is invalid now.</li><li>Transferring the modified data among servers: When many reads and few updates, each server can receive latest version immediately in this way.</li><li>Don’t send modified data, but modification commands: When commands substantially smaller than data, it’s good to use this method, also good when network bandwidth the scarse.</li></ol><p>Pull and Push based updates:</p><ul><li>When high degree of consistency is required, push-based updates is better. Servers can push updates to replica servers, mostly used in server-initiated replica setups.</li><li>When read-to-update ratio is low, pull-based updates is better. Clients request updates from server, often used by client caches.</li></ul><h5 id="How-to-deal-with-inconsistencies"><a href="#How-to-deal-with-inconsistencies" class="headerlink" title="How to deal with inconsistencies?"></a>How to deal with inconsistencies?</h5><p>When data in different locations, inconsistency can occur</p><p>Higher the consistency level, lower the performance</p><p>There must be a clear understanding what CL an app can expect from a replicated data store.</p><p>Consistency models: Contract between client and store</p><p>Two views of consistency models:</p><ol><li>Data-centric consistency models: from a global perspective, provides guarantees how a sequence of r/w operations are perceived by multiple clients</li><li>Client-centric consistency models: from a client’s perspective, provides guarantees how the state of a replicated data item is perceived by a single client.</li></ol><p>Data-centric consistency models:</p><ul><li><p>Strong consistency models: operations on shared data is synchronized (Strict\ Sequential \ Causal  Consistency)</p></li><li><p>Weak consistency models: Synchronization only when data is locked/unlocked (General week\ Release\ Entry Consistency)</p><p>​</p><p>Strict Consistency: any read to a shared data item x returns the value stored by the most recent write operation on x.</p></li></ul><p>​            (if follow strict consistency, client 2 read x just after client 1 write x. If don’t follow, client 2 can read another value of x which was before x wrote by client 1, 任何读操作都能读取到最新的修改，换句话说，要求任何写操作都立刻同步到其他所有进程。这里强调的是绝对时钟上的立刻同步，而任何信息的同步都需要延迟，因此在分布式系统下无法严格实现。)</p><p>​    ❓ Sequential Consistency: The result of any executions is the same as if the r/w operations of all processes were executed in some sequential order, and the operations of each individual process appear in this sequence in the order specified by its program.</p><p>Sequential Consistency将所有事件定一个全序，且单个进程内的多个操作满足program order，每个进程看到的都是这个全序。只要每个进程看到的定序结果是一致的，系统就是满足Sequential Consistency的。右图中client3和client4两个进程看到的读a和b的顺序不一致。</p><p>​    Causal Consistency:</p><p>Causal Consistency要求，如果两个事件有因果关系，则要求这两个事件的先后顺序满足因果序，并且所有进程看到的这两个事件的顺序都是满足这个因果序的。</p><p>Causal Consistency相比Sequential Consistency来说，仅要求有因果关系的事件顺序对所有进程看到的一致，没有因果关系的事件顺序对于所有进程可以不一致。</p><p>右图中client1对a的写操作和client2对b的写操作有因果关系，因此client3和client4读的时候要求顺序一致。</p><p>Client-centric consistency models:</p><p><a href="http://mark311.github.io/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7/2014/10/18/eventually-consistent.html" target="_blank" rel="external">client-centric consistency</a></p><p>Eventual consistency: All replicas will eventually reach the most recent state, but clients may read old data and loose its own updates, implemented cheaply. 最终一致性，就是不保证在任意时刻任意节点上的同一份数据都是相同的，但是随着时间的迁移，不同节点上的同一份数据总是在向趋同的方向变化。也可以简单的理解为在一段时间后，节点间的数据会最终达到一致状态</p><p>1) Monotonic reads:</p><p>If a process reads x, any future reads on x by the process will returns the same or a more recent value</p><p>读到的数据总是不旧于上一次读到的数据。此种一致性要求如果Process A已经读取了对象的某个值，那么后续操作将不会读取到更早的值。</p><p>2) monotonic writes: 系统保证写操作由同一个进程执行。编写不提供这种一致性级别保证的系统是众所周知的困难</p><p>A write by a process on x is completed before any future write operations on x by the same process</p><p>3) Read your writes: 进程A更新一个数据项之后，再去访问它，总能得到更新后的值，并且不再会看到这个数据项更新之前的值。这是causal consistency模型的特殊形式。</p><p>A write by a process on x will be seen by a future read operation on x by the same process</p><p>4) writes follow reads:</p><p>A write by a process on x after a read on x takes place on the same or more recent value of x that was read</p><p>In general, the stricter the consistency model, the more it impacts the scability of a system.</p><ul><li>More consistency requires more synchronization</li><li>While the data is synchronized, some client requests may be answered</li></ul><p>Today’s cloud databases often sacrifice consistency for more scalability and availability.</p><h4 id="Brewer’s-CAP-theorem"><a href="#Brewer’s-CAP-theorem" class="headerlink" title="Brewer’s CAP theorem"></a>Brewer’s CAP theorem</h4><p>In a distributed system, it is <strong>impossible</strong> to provide three guarantees at the same time:</p><ul><li>Consistency: Write to one node, read from another node will return something no older than what was written</li><li>Availability: Non-failing node will send proper response</li><li>Partition tolerance: keep promise of either consistency or availablity in case of network partition.</li></ul><p>Illustrations of CAP:</p><ol><li>CA: provided all servers can communicate, e.g. Replicated DBMS</li><li>PC: System can provide even if some nodes are temporarily unreachble. e.g. pessimistic locking of distributed db.</li><li>AP: System tolerates network outages, it can still work even when some DNS servers are offline. e.g. DNS system</li></ol><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><h4 id="Amazon-Dynamo"><a href="#Amazon-Dynamo" class="headerlink" title="Amazon Dynamo"></a>Amazon Dynamo</h4><p>Highly-available key-value store used inside Amazon</p><p>Sacrifices consistency to achieve high scalability, high availability, high performance and small latency.</p><p>Design principles:</p><ol><li>Follow Peer-To-Peer approach: every server is equal important, no single point of failure</li><li>Nodes can be added/removed incrementally at runtime: AP, weak consistency(eventual)</li><li>No hostile environment, all servers obey rules: no need to consider security issues.</li></ol><p>How to partition data among servers?</p><ul><li>​key-value store ➡️ no support for range queries needed, no need for range partitioning</li><li>hash partitioning has good load palancing properties</li><li>To avoid data reorganization when servers are added/removed, hash function no longer maps to partitions, instead functions maps to a fixed number of slots, it’s called <em>consistent hashing</em></li></ul><p>Consistent hashing: Additional mapping between slots and partitions</p><p>Distributed Hash tables</p><ul><li>each server takes at least one partition, therefore each server is responsible for a continuous range of slots, add/remove server only O(#key/#servers) data items must be reorganized</li></ul><p>:impressed: Amazon Dynamo’s partitioning algorithm: </p><p>To distribute data, key of data also hased with MD5, result of hashing is a position in the circular ID space.</p><p>Each server maintains full routing table (ID to IP)</p><ul><li>each server can determine which server is responsible for a data item based on routing table and mapping rule</li><li>one hop routin keeps latencies small</li></ul><p>Despite uniform distribution over ID space, the servers may receive skewed number of requests</p><p>:lightning: each server appears on multiple positions on the ring</p><p>Replication in Amazon Dynamo</p><ul><li>Servers replicate data to their N successors on the ring</li><li>r/w are allowed on every replica</li><li>replication is adjusted when a server is added/removed</li><li>Use heartbeat protocol to determine availability: periodic msg exchanged between ring neighbors, if a server doesn’t answer heartbeat request in a given time, it is considered gone.</li></ul><p>Data Version Reconciliation</p><p>Because Dynamo allows “always write” paradigm, different version of a data item may exist. Vector clocks are used to reconcile different versions.</p><h4 id="Microsoft-Azure"><a href="#Microsoft-Azure" class="headerlink" title="Microsoft Azure"></a>Microsoft Azure</h4><p>this PaaS platform was consisted of:</p><ul><li>Microsoft Azure: compute and storage services</li><li>SQL Azure: cloud-based DBMS</li><li>Azure AppFabric: tools to bridge gap between local and cloud-hosted apps</li></ul><p>Programming model: </p><ol><li>Azure applications separated into logical components, each components as assigned to a role. There are three roles:<ul><li>Web role: components facing outer world, accept requests via HTTP</li><li>Worker role: components doing background tasks</li><li>VM role: legacy components, component which cannot be converted to the pogramming model</li></ul></li><li>Each instance of a component is executed in a separate VM<ul><li>to improve maintainability and isolation</li><li>customer can choose between different types of VM</li><li>VM usage is billed by hour</li></ul></li></ol><p>Why different roles?</p><ul><li>To provide different levels of abstraction</li><li>But hardware and Microsoft Azure Hypervisor are the same across all roles</li></ul><p>Azure is basically a standard Windows environment, every programming language for Windows Server also runs on Azure.</p><p>Entry points into components:</p><ul><li>VM role entry point: precompiled VM image, Azure is agnostic(不可知的) of guest OS</li><li>Worker role entry point: Archive with intermediate code, code must implement particular interface</li><li>Web role entry point: Archive with web code</li></ul><p>How to achieve scalability?</p><ul><li><em>Run many instances of each role</em>, but instances of roles must be stateless.</li><li>Load balancers distribute requests between instances<ul><li>For web roles: HTTP load balancer for requests(round-robin distribution)</li><li>For worker roles: dependant on communication scheme</li></ul></li></ul><p>Azure offers distinct services to store state: Azure storage</p><ul><li>Table: abstraction similar to Excel sheet: <ul><li>Set of entities which are basic dataitems which composed of properties, which is part of an entity</li><li>Tables are partitioned horizontally, reads are load balanced across three replicas</li><li>Strong consistency</li><li>cost depend on total space occupied on storage services</li><li>Maximum size of a table is 100TB, Maximum size of entity is 1MB</li></ul></li><li>Blob: Key/value store for BLOB<ul><li>BLOB: single data item</li><li>Container: set of BLOBs</li><li>Max size of BLOB is 100TB, max size of a single BLOB is 50GB</li><li>Strong consistency</li><li>price similar to table storage</li></ul></li><li>Queue: Reliable msg delivery service between roles<ul><li>Does not gurantee FIFO and msg can be returned more than once</li></ul></li><li>SQL Azure: based on MS SQL Server<ul><li>Limited scalability compare to Table storage: size of db limited to 50 GB</li><li>Strong consistency</li><li>Billing per GB per month</li></ul></li></ul><p>Maintanence of Azure platform</p><p>​    deal with OS patches, middleware updates…</p><ul><li>Bring up new instance of role on new HW / patched OS image</li><li>Redeploy customer’s code</li><li>Register new instance with the load balancer</li><li>kill outdated instance</li><li>continue until all old instances have been replaced</li></ul><p>Three rules of the Azure programming model</p><ol><li>An Azure app is built from one or more roles</li><li>An Azure app runs multiple instances of each role: key to scalability and availability, allow Microsoft to silently update and restart instances</li><li>An Azure app behaves correctly when any role instance fails</li></ol><h4 id="Google-App-Engine"><a href="#Google-App-Engine" class="headerlink" title="Google App Engine"></a>Google App Engine</h4><p>Provide good response times to web clients</p><p>Provide APIs of multiple language to write and deploy web players</p><p>Automatic scale-out and scale-down according to changes of requests number</p><ul><li>Applications must not maintain internal state</li><li>Store any data in cloud storage services</li></ul><p>App Engine Datastore</p><ul><li><p>Reliable data store for key-value structured data</p></li><li><p>Builds upon two components:</p><ul><li>Google File System(GFS): distributed, scalable, fault-tolerant</li><li>Google Bigtable: flexibl, distributed storage for structured data</li></ul></li><li><p>GFS was designed to meet scalability, high performance, support for commodity HW, fault-tolerance</p></li><li><p>GFS design principles:</p><ul><li>limited support for random writes</li><li>data can be efficiently appended</li><li>sits on top of regular file systems</li><li>custom API to the developer</li></ul></li><li><p>GFS architecture</p><p>master-worker architecture. Master stores metadata, workers store actual data, called chunk server. Files are split into fixed-sized chunks, replicated across machines</p><p>When reading a file X from GFS:</p><ol><li>Client knows chunk size, converts offset to chunk index</li><li>client contacts master with filename and chunk index</li><li>master returns chunk handle and list of replicas</li><li>client sends read request to closest replica</li><li>client caches chunk handle so further reads of the same chunk require no more client-master interaction until cached information expires or file X is reopened.</li></ol><p>In this architecture, Master has global knowledge of the file system, which simplifies design and response times, make easy garbage collection and data reorganization.</p><p>Metadata is kept in main memory for performance, for each chunk, it consumes 64 bytes.</p><p>Chunk size is an important parameter of GFS, it determines the amount of metadata that fits into memory, and the frequency of the client requests.</p><p>If master fails, namespace/file-to-chunk mappings are written to log, this operation log is persistent on master’s hard disk and replicated to remote machine. Replica locations are requested from chunk servers when the master starts or new chunk server joins</p><p>Servers as logical timeline that defines the order of concurrent operations. Files and chunks are all uniquely and eternally identfied by their logical times of creation. Changes of file system are only visible to clients after the log has been flushed locally and remotely.</p><p>Master can recover FS state by replaying log.</p><p>A mutation (w/a) is performed at all the chunk’s replicas</p><p>Master grants chunk lease to one replica to ensure consistent mutation order across all the replicas</p><p>Primary choose serial order for mutations</p><p>GFS support for atomic appends. </p><p>​    Append is writing to a file at specific offset which traditionally specified by client. But in presense of two application concurrently specify to write record at offset Y, one record would be destroyed.</p><p>​    Special GFS operation to append data in atomic units. Operation guarantees that data is appended at least once. procedure:</p><ol><li><p>Client specifies append operation and data (but no offset!!)</p></li><li><p>Primary serializes requests, chooses offsets</p></li><li><p>returns offsets to client after data has been appended.</p><p>Data appended to next chunk if size exceeded otherwise</p><p>Client retries operation if appendent fails at any replica</p></li></ol></li><li><p>Google Bigtable</p><p>GFS offers tremendous storage capacity, but limitedd support to efficiently retrieve structured data.</p><p>​Solution ➡️ Google bigtable</p><ul><li>multi-dimensional sorted map</li><li>builds on top of the GFS</li><li>many use cases, backend for App Engine Datastore</li></ul><p>No schema, each row can have arbitrary columns, columns are grouped to column families.</p><p>Bigtable clusters stores several ables, a table consists of a set of tablets, tablet contains all data associated with a row range, each table has one tablet intially, when tablet grows, it is automatically split into several tablets.</p><p>Bigtable architecture:</p><p>​    Client sending requests (row, column, time), expecting value</p><p>​    Master can manage serveral tables, assigning tablets to tablet servers, keep track of addition/expiration of tablet servers, garbage collection, load balancing</p><p>​    tablets servers store actual tablets, serve client requests</p><p>Three-level hierarchy to locate correct tablet, client specifies table and row key, first lookup in root tablet. Information is stored in special METADATA tables.</p><p>Internal Tablet Structure:</p><p>​    Actual data is stored in a SSTable(sorted string table)</p><p>​    each SSTable consists of several blocks and index</p><p>Updates are stored to a commit log in GFS. Reads are served by merging memtable and SSTables</p><ul><li>Minor compaction: convert membtable into an SSTable, reduces memory usage and reconstruction effort of memtable on restart</li><li>Mergin compaction: reduce number of SSTables, happens periodically in the background</li><li>Major compaction: Merging compaction that results in only one SSTable, special case of merging compaction that purges deleted data.</li></ul></li></ul><h2 id="Cloud-computing-chapter-4-Data-Intensive-Applications-on-cloud-Architectures"><a href="#Cloud-computing-chapter-4-Data-Intensive-Applications-on-cloud-Architectures" class="headerlink" title="Cloud computing chapter 4 - Data-Intensive Applications on cloud Architectures"></a>Cloud computing chapter 4 - Data-Intensive Applications on cloud Architectures</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Amount of digitally available data grows rapidly</p><h4 id="Relation-Data-Intensive-Applications-clouds"><a href="#Relation-Data-Intensive-Applications-clouds" class="headerlink" title="Relation Data-Intensive Applications -clouds"></a>Relation Data-Intensive Applications -clouds</h4><ul><li>Requirements for data-intensive applications outscaled several traditional solutions: parallel databases are prohibitively expensive</li><li>Large sets fo commodity clusters are preferred: Data is stored on local hard disks, allows to keep computation close to the data</li><li>Individual commodity servers are less reliable. Data-intensive applications are typically elastic</li><li>Cloud computing often considered promising platform for data-intensive applications: looks like a large pool of clusters and customer, elastic platform, no large upfront capital expenses</li><li>However, virtualization overhead for I/O operations, no control over physical infrastructure</li></ul><p>Challenges of large-scale data processing</p><ul><li>Large clusters/clouds have 100s/1000s of servers, problem: high parallel environment</li><li>Writing efficient parallel applications at this scale is hard</li><li>needed: suitable abstraction layer for developers</li></ul><p>Abstraction layer for data-intensive applications:</p><ol><li>Developers don’t have to think about parallelization</li><li>Developers don’t have to think about fault tolerance</li><li>Developers don’t have to think about load balancing</li></ol><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>Map and reduce take first-order functions as input, specify how data is passed to first-order function.</p><p>MapReduce operates on a key-value model.</p><p>Map function: first-order function provided by user, which specifies what happens to the data in job’s map phase</p><ul><li>signature: $(k_1, v_1) \rightarrow list(k_2, v_2)$</li><li>first-order functions is invoked once for each KV pair</li><li>useful for projections, selection…</li></ul><p>Reduce function: First-order function provided by user, specifies what happens to the data in job’s reducephase</p><ul><li>signature: $(k_2, v_2) \rightarrow list(k_2, v_2)$</li><li>all kv-pairs with the same key are presented to the same invocation of the first-order function</li><li>useful for aggregation, grouping…</li></ul><p>Mapper: a process running on a worker node, invokes map function for each KV pair</p><p>Reducer: process invoking reduce function on grouped data</p><h4 id="MapReduce-Implementation"><a href="#MapReduce-Implementation" class="headerlink" title="MapReduce Implementation"></a>MapReduce Implementation</h4><ul><li>Designed to run on large set of shared nothing servers</li><li>expects distributed file system, every node can potentially read every part of input</li><li>Individual nodes are likely to fail</li><li>Prefers local storage</li></ul><p>MapReduce follows master-worker pattern</p><ul><li>Master: responsible for job scheduling, monitor worker nodes, detect dead nodes, load balancing</li><li>Workers: execute map and reduce functions, store input/output data, periodically report availability to master node</li></ul><h4 id="Distributed-Execution-of-MapReduce-program"><a href="#Distributed-Execution-of-MapReduce-program" class="headerlink" title="Distributed Execution of MapReduce program"></a>Distributed Execution of MapReduce program</h4><ol><li>client partitions input file into input splits: splits define maximal scale-out, independent of GFS block size</li><li>Client submits job to master: includes code of map/reduce functions and list of input splits, master tries to find free resources to schedual mappers/reducers</li><li>Mapper started for each input splits: executing the user’s map function, multiple mappers per server possible, servers with replicas of input data preferred by the master, intermediate output of mappers is partitioned by intermediate key.</li><li>Reducers pull data from mappers over network</li></ol><p>refinements:</p><ul><li>can be used to locally aggregate intermediate results: executed after the mappers, it’s useful to unburden network. e.g. Example: If intermediate KV pair <for, 1="">, <for, 1=""> is created by the same mapper, the combiner aggregates it to <for, 2="">, without combiner <for, (1,1)=""> is shipped</for,></for,></for,></for,></li><li>Applicability of combiner depends on job</li></ul><h4 id="MapReduce-Fault-Tolerance"><a href="#MapReduce-Fault-Tolerance" class="headerlink" title="MapReduce Fault Tolerance"></a>MapReduce Fault Tolerance</h4><ul><li>if mapper fails: master detects failure through missing status report, mapper is restarted on different node, re-reads data from GFS</li><li>if reducer fails: detect through missing status report, reducer is restarted, pulls intermediate results for its partition from mappers again</li><li>entire worker fails: master re-schedules lost mappers and reducers, finished mappers may be restarted to recompute lost intermediate results</li></ul><h4 id="MapReduce-limitations"><a href="#MapReduce-limitations" class="headerlink" title="MapReduce limitations"></a>MapReduce limitations</h4><ol><li>assumes finite input which prevents streaming process, useful to respond to events without large delays</li><li>data between MR jobs must go to Google File System: detrimental for iterative algorithms</li></ol><h4 id="Map-Reduce-with-Stratosphere"><a href="#Map-Reduce-with-Stratosphere" class="headerlink" title="Map/Reduce with Stratosphere"></a>Map/Reduce with Stratosphere</h4><p>❓ how to improve the efficiency of massively parallel data processing on IaaS platforms?</p><p>extend elasticity, but face challenges that loss of control due to required virtualization</p><p>Stratosphere features:</p><ol><li>exploiting the cloud’s elasticity</li><li>detecting parallelization constraints</li><li>reduce I/O bottlenecks</li><li>Inferring physical network topologies</li></ol><h4 id="Amazon-Elastic-MapReduce"><a href="#Amazon-Elastic-MapReduce" class="headerlink" title="Amazon Elastic MapReduce"></a>Amazon Elastic MapReduce</h4><p>Cloud service for data-intensive applications introduced in 2009 as part of AWS</p><p>Job processing cycle on Amazon EMR</p><ol><li>Customer submits job through web interface:<ul><li>Job specification contains: location of input data on Amazon S3, MapReduce code, user libraries, parameters…, number virtual machiens to run the job, type of VM to run the job, Designated output location on Amazon S3</li></ul></li><li>Requested VM are started on EC2:<ul><li>Pricing model starts</li><li>Booted AMIs to preconfigured to start HDFS/Hadoop</li><li>Hadoop worker nodes automatically contact master</li></ul></li><li>MapReduce job is processed on rented VMs</li><li>After completion, EC2 instance are automatically shut down</li></ol><p>EMR and Elasticity</p><ul><li>EMR allows customers to adjust number of VMs while job is running, customer can monitor job</li></ul><p>​Problem:lightning: But VMs to be removed may store intermediate results in HDFS</p><p>➡️ HDFS expects node loss as result of HW failure. </p><p>Solution: EMR separates VMs in three distinct groups.</p><ol><li>Master group: contains only VM running MR master</li><li>Core group: VMs run Hadoop worker and HDFS node, the size of core group only can be increased</li><li>Task group: VMs only Hadoop worker, VMs store no local data, intermediate data is transferred to core group, size is flexible</li></ol><h3 id="Flink-and-Spark"><a href="#Flink-and-Spark" class="headerlink" title="Flink and Spark"></a>Flink and Spark</h3><p>MapReduce evolutions:</p><ul><li>need more operator: join, groupBys, filter… besides map and reduce</li><li>to simplify a wide array of computation: iterative ml, streaming, complex batch jobs</li><li>keep intermediate results in memory</li><li>Generalize processing engine to handle streams: use batch engine with small batch size/ use real streaming engine</li><li>Stream processing requires redefinition of operators: use window operators replace reduce function</li></ul><h4 id="Apache-Spark"><a href="#Apache-Spark" class="headerlink" title="Apache Spark"></a>Apache Spark</h4><h5 id="RDDs"><a href="#RDDs" class="headerlink" title="RDDs"></a>RDDs</h5><p>At Spark’s core are parallel transformations of Resilient Distributed Datasets(RDDs).</p><ul><li>RDDs are read-only collections of objects, distirbuted and partitioned across nodes, in-memory</li><li>Bulk operations transform RDDs in parallel on workers</li><li>RDDs are logical (lazy and ephemeral): run when they are used, contains enough information to compute it starting from data in reliable storage.</li><li>Fault-tolerance</li><li>cacheing: users can give command to keep RDDs in -memory</li><li>enables faster iterative jobs</li></ul><h5 id="Spark-streaming"><a href="#Spark-streaming" class="headerlink" title="Spark streaming"></a>Spark streaming</h5><p>spark processes continuous inputs as Microbatches</p><ul><li>arriving input is processed in batches which can be windows for operations.</li></ul><h4 id="Apache-Flink"><a href="#Apache-Flink" class="headerlink" title="Apache Flink"></a>Apache Flink</h4><p>A project to unify batch and stream processing in one engine. Flink’s core supports batch and streaming functions.</p><h5 id="Flink-execution-model"><a href="#Flink-execution-model" class="headerlink" title="Flink execution model"></a>Flink execution model</h5><p>A job consists of a directed acyclic graph(DAG) of oeprators and Intermediate streams of data records and control events flowing through the DAG of operators</p><p>follows a master-slave paradigm: job managers keeps track of all Task Managers and job execution.</p><p>A task manager will have one or more task slots.</p><p>complete DAG of operators are deployed distributed on all task managers: A task slot executes a pipeline of tasks, often execute successive operators concurrently</p><h5 id="Performance-comparison-of-streaming-approaches"><a href="#Performance-comparison-of-streaming-approaches" class="headerlink" title="Performance comparison of streaming approaches"></a>Performance comparison of streaming approaches</h5><ul><li><p>Micro batches: processing of small batches of tuples</p></li><li><p>“real” streaming: tuple-wise processing</p></li><li><p>Flink has lower latency: it processes an event as it becomes available</p></li></ul><h2 id="Cloud-computing-chapter5-Continuous-Integration"><a href="#Cloud-computing-chapter5-Continuous-Integration" class="headerlink" title="Cloud computing chapter5 - Continuous Integration"></a>Cloud computing chapter5 - Continuous Integration</h2><p>Developers(Dev) develop without knowledge of productive infrastructure</p><p>Operators(Ops) push code as a black box as they are not involved in development</p><p>​:lightning: conflicts: failure in server or in development?</p><p>➡️ Integrate both processes into continuous process</p><h3 id="Continuous-integration"><a href="#Continuous-integration" class="headerlink" title="Continuous integration"></a>Continuous integration</h3><p>Continuous Integration: integrating changes from different developers in the team into a mainline as early as possible</p><p>Continuous Development: keeping the application deployable at any point or even automatically releasing to a test or production. </p><p>Continuous Delivery: keeping the codebase deployable at any point and have all the configuration necessary to push it into production</p><p>■ Developers integrate code into a shared repository frequently (several times a day)<br>■ Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible<br>​■ Small changes introduced to code ➡️ Quick detection and localization of failures possible</p><p>Principles of continuous integration:</p><ul><li>Maintain a single source repository which keeps track of all changes</li><li>Automate the builds</li><li>Make the build self-testing and keep the builds fast</li><li>Daily commits to the baseline by everyone on the team</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Cloud-Computing-Chapter1-–-Introduction&quot;&gt;&lt;a href=&quot;#Cloud-Computing-Chapter1-–-Introduction&quot; class=&quot;headerlink&quot; title=&quot;Cloud Computing Chapter1 – Introduction&quot;&gt;&lt;/a&gt;Cloud Computing Chapter1 – Introduction&lt;/h1&gt;&lt;h2 id=&quot;Concepts-of-Cloud-Computing&quot;&gt;&lt;a href=&quot;#Concepts-of-Cloud-Computing&quot; class=&quot;headerlink&quot; title=&quot;Concepts of Cloud Computing&quot;&gt;&lt;/a&gt;Concepts of Cloud Computing&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;✴️ Definitions of Cloud Computing according to NIST ✴️&lt;/strong&gt;:  Cloud computing is a model for &lt;em&gt;enabling ubiquitous, convenient , on-demand network access&lt;/em&gt; to a shared pool of configurable computing resources that can be &lt;em&gt;rapidly provisioned and released with minimal management effort or service provider interaction&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="CloudComputing" scheme="http://yoursite.com/tags/CloudComputing/"/>
    
  </entry>
  
  <entry>
    <title>Distributed Algorithms C7 -- Consistent snapshot</title>
    <link href="http://yoursite.com/2017/03/07/DAlg-notes-C7/"/>
    <id>http://yoursite.com/2017/03/07/DAlg-notes-C7/</id>
    <published>2017-03-07T08:34:22.000Z</published>
    <updated>2017-03-08T08:44:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>• What is the problem with global snapshots?<br>• What is a global state?<br>• Which snapshot algorithms do you know?</p><p>snapshot problem</p><p>consistency criterion for consistent cuts</p><p>snapshot algorithms</p><p>snapshot applications</p><a id="more"></a><h2 id="The-snapshot-problem"><a href="#The-snapshot-problem" class="headerlink" title="The snapshot problem"></a>The snapshot problem</h2><p>Determine “current” snapshot of the global state without stopping the system.</p><p>One cannot catch all processes at the same time, messages that are on the way cannot be seen.</p><p>The determined state should at least be <strong>consistent</strong>, the saved state should not be influenced by future messages.</p><h3 id="Global-state"><a href="#Global-state" class="headerlink" title="Global state"></a>Global state</h3><p>The execution of each process in the distributed system can be characterized by its history: $history(p_i) = h_i=<e^0_i,e^1_i,e^2_i,...>$ .</e^0_i,e^1_i,e^2_i,...></p><p>We define a prefix of the process history as $h^k_i=<e^0_i,e^1_i,... e^k_i="">$, define $s_i$ as the state of the process $p_i$, $s^k_i$ denotes the state of $p_i$ immediately before the k-th event.</e^0_i,e^1_i,...></p><p>The global history $H$ is the union of the individual process histories, $H=h_1 \cup h_2 \cup … \cup h_N$, a global state $S$ is represented by any set of individual process states.</p><p>A <em>Cut C</em> of the system’s execution is a <em>union of prefixes</em> of process histories, $ C=h_1^{c1} \cup h^{c2}_2 \cup … \cup h^{cn}_N$ </p><p>A meaningful global state is <em>represented by a consistent cut</em>:</p><ul><li>cut C contains all events up to $e^{ci}_i$</li><li>State $s_i$ of each process contained in global state $S$ corresponding to the Cut C is that$p_i$ immediately after the last event $e^{ci}_i$ has been processed.</li><li>The set of last events of the individual process prefixes is called frontier.</li><li>a cut is inconsistent if it shows an effect without cause.</li></ul><p>A Cut C is consistent if it also contains all events that happened-before for each of this cut’s event. $ \forall e \in C, f \rightarrow e \Rightarrow f \in C$ </p><p>一个切割把进程线上的事件分成两个部分，如果切割线不跨越通信线，那么是一个一致性切割，如果跨越了通信线，但两个切割事件没有因果关系，也不会导致切割的不一致，对于正在传送中的消息是一致但非强一致的状态。</p><h3 id="snapshot-algorithms"><a href="#snapshot-algorithms" class="headerlink" title="snapshot algorithms"></a>snapshot algorithms</h3><p>purpose:</p><ul><li>provide a potential consistent past global state</li><li>global predicates can only be evaluated by means of consistent snapshots</li><li>a predicate is stable if it continues to hold after it applied once</li><li>a potential past state is useful for the detection of stable predicates.</li></ul><p>Lai and Yang’s snapshot algorithm:</p><ol><li>initially, all nodes are black and send black messages.</li><li>The initiator becomes red and stores its local state</li><li>red nodes only send red messages</li><li>other nodes become red, if they receive an order to snapshot or a red message</li><li>before a node becomes red, it saves its local state and sends it to the initiator</li><li>if a red node receives a black message, it sends a copy of the message to the initiator.</li><li>The snapshot is complete if the initiator has received the local states of all nodes, and a copy of each black message that was on the way.</li></ol><p>让所有进程进入snapshot状态并记录当前的global state。首先initiator发送消息给其它进程，其它进程收到initiator的red message后开始记录自己 local state发送给initiator，然后变红。如果红点收到黑消息，就发送一份copy给initiator</p><p>Chandy and Lamport algorithm:</p><p>use flooding as basic wave procedure.</p><ul><li>A control message “pushes” the black messages to go to the FIFO channels</li><li>If a node receives a control message over a channel, then it knows it won’t receive more black message over that channel.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Marker receiving rule for process pi</div><div class="line">Pi received a marker msg from channel c:</div><div class="line">if (pi has no yet recorded its state):</div><div class="line">it records its process state now</div><div class="line">it records the state of c as the empty set</div><div class="line">it runs on recording msg over other incoming channels</div><div class="line">else:</div><div class="line">pi stop record c&apos;s state and take the message it has recieved before as the state of c.</div><div class="line"></div><div class="line">Marker sending rule for process pi</div><div class="line">after pi has recorded its state, for each outgoing channel c:</div><div class="line">pi sends one marker message over c before it sends any other message over c</div></pre></td></tr></table></figure><p>进程1想知道全局状态，先向其他进程发起marker消息，并开始记录发送过marker消息的信道状态。</p><p>当进程从信道c第一次收到marker时，开始记录该信道状态，并向其它信道发送marker。</p><p>弱进程从信道c第二次收到marker，则停止记录该信道状态，并将已记录的消息座位该信道状态。</p><h2 id="Distributed-termination-detection"><a href="#Distributed-termination-detection" class="headerlink" title="Distributed termination detection"></a>Distributed termination detection</h2><p>Asynchronous model: process are active or passive, determine whether all processes are passive and no messages are on the way at the certain point of time.</p><p>Process model: determine whether all process are passive at a certain point in time, because message in this model have no delay.</p><p>Atom model: Actions are atomic and need no time, determine whether no messages are on the way at a certain point in time.</p><p>Simple counting algorithm: An observer visits each node and separately sums up the basic messages sent and received, when the number of sent messages and received messages are same, then it may terminate.</p><p>But: it is not sufficient for termination, because due to the cut, the message may be sent in the future received in the past.</p><p>Solution:</p><ol><li><p>freezing the system:</p><ul><li>no messages are sent in the frozen system</li><li>then sums up the messages, if both sums are equal, terminate</li><li>unfreezes system</li></ul><p>drawback: decrease concurrency</p></li><li><p>Double counting algorithm</p><ul><li>An observer twice visits all nodes, and determine the respective sums of messages received and sent</li><li>If 4 sums are equal, terminate</li></ul><p>if termination was not detected, second wave as first wave of the new round, re-entrant.</p></li><li><p>control topologies: different ways of waves for termination</p><ul><li>sequential waves: use a logical ring and two subsequent sequential ring circuits, $O(n)$</li><li>parallel waves: span tree and two subsequent accumulations each from leafs to root $O(log n)$</li><li>usage of echo algorithm</li></ul></li><li><p>time zone algorithm</p><p>An observer visits all nodes and builds a send and receive sum respectively</p><p>​</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;• What is the problem with global snapshots?&lt;br&gt;• What is a global state?&lt;br&gt;• Which snapshot algorithms do you know?&lt;/p&gt;
&lt;p&gt;snapshot problem&lt;/p&gt;
&lt;p&gt;consistency criterion for consistent cuts&lt;/p&gt;
&lt;p&gt;snapshot algorithms&lt;/p&gt;
&lt;p&gt;snapshot applications&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="Distributed Algorithm" scheme="http://yoursite.com/tags/Distributed-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Distributed Algorithm C6 Clocks</title>
    <link href="http://yoursite.com/2017/03/05/DAlg-notes-C6/"/>
    <id>http://yoursite.com/2017/03/05/DAlg-notes-C6/</id>
    <published>2017-03-05T08:34:20.000Z</published>
    <updated>2017-03-08T08:40:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>• Why do we need clocks in distributed systems?<br>• What’s the problem anyway, ever computer in the system has a clock…<br>• Synchronization of physical clocks, how do we realize this?<br>• What are Logical Clocks, and what kind of logical clocks do you know?</p><p>Time in Distributed Systems</p><p>Synchronization of physical clocks</p><ul><li>External clock synchronization after Cristian</li></ul><p>Order of events</p><p>Logical clocks</p><ul><li>Lamport clocks</li><li>vector clocks</li><li>application of vector clocks: causal broadcast</li></ul><a id="more"></a><h2 id="Time-in-Distributed-Systems"><a href="#Time-in-Distributed-Systems" class="headerlink" title="Time in Distributed Systems"></a>Time in Distributed Systems</h2><p>A clock maps the real time $t$ on a time stamp $C(t)$</p><p>Resolution: smallest period of time by which two values of the clock can differ.</p><p>Drift: Deviation of the speed of the clock from real time.</p><p>offset: Deviation of the clock from the real time at a point in time. i.e., $t-C(t)$</p><p>Each computer has its own inaccurate digital clock, the drifts are different from each other, without synchronization, the values of the clocks can differ arbitrarily from each other, thus we need <em>clock synchronization</em></p><h2 id="Synchronization-of-physical-clocks"><a href="#Synchronization-of-physical-clocks" class="headerlink" title="Synchronization of physical clocks"></a>Synchronization of physical clocks</h2><h3 id="Synchronization-interval"><a href="#Synchronization-interval" class="headerlink" title="Synchronization interval"></a>Synchronization interval</h3><p>Two correct clocks with drift $\rho$ should not deviate by more than $d$. </p><p>If the clocks are synchronized at $t=0$, then $C_1(t)=(1+\rho)t$  (faster) and $C_2(t)=t / (1+\rho)$ (slower) <strong>???为啥不是$1-\rho$ ???</strong></p><p>$(1+\rho)t - \frac{t}{1+\rho} \le d$ , the clock must be synchronized again before $d\frac{1+\rho}{2\rho + \rho^2}$ </p><p>If $\rho$ is very small, synchronization every $d/2\rho$ </p><h3 id="External-clock-synchronization-Cristian"><a href="#External-clock-synchronization-Cristian" class="headerlink" title="External clock synchronization (Cristian)"></a>External clock synchronization (Cristian)</h3><p>For known message delays $x$, if process $P_2$ wants to adjust its clock based on process $P_1$, it just need to use $P_1$ ‘s time $t_1$ of sending message, plus the message delay $x$, then subtract $P_2$ ‘s time $t_2$ when receives the message, after that we know how  much time we need to adjust $P_2$’s clock.</p><p>For unforeseeable message delay, the preceding procedure leas only to an approximate adjustment.</p><p>Requirements:</p><ul><li><p>great leaps of the clock time shall be avoided</p></li><li><p>the clock time must not decrease</p><p>The local clock runs slower or faster until the offset is compensated.</p></li></ul><p>Berkeley Algorithm: Assume clocks are correct, calculate the average of all clocks as the measure to adjustment.</p><h2 id="Order-of-Events"><a href="#Order-of-Events" class="headerlink" title="Order of Events"></a>Order of Events</h2><h3 id="definitions"><a href="#definitions" class="headerlink" title="definitions"></a>definitions</h3><p>Sending and receiving of a message are events. In distributed systems, the absolute point in time of events are often not important, we care about the order of events.</p><p>Partial order: the order relation is not defined for all pairs of events.</p><p>Total order: the order relation is defined for all pairs of events. i.e. $e_1 &lt; e_2 \rightarrow e_1 &lt; e_2 \vee e_2 &lt; e_1$</p><h3 id="possible-order-requirements"><a href="#possible-order-requirements" class="headerlink" title="possible order requirements"></a>possible order requirements</h3><ol><li><p>FIFO. If a process sends $m_1$ before $m_2$, then $m_1$ is delivered before $m_2$ to the receiver</p></li><li><p>casual order. A received message might cause the receiver to send another message, while another process may get these two messages in wrong order. </p><p>=&gt; we can keep causalities based on “happened before” relation.</p></li></ol><h3 id="“Happened-before”-Relation-Lamport"><a href="#“Happened-before”-Relation-Lamport" class="headerlink" title="“Happened before” Relation (Lamport)"></a>“Happened before” Relation (Lamport)</h3><p>Use the relation $\rightarrow$ to fulfill two following conditions:</p><ul><li>If $a$ and $b$ are two events in a process and $a$ occurs before $b$, then $a \rightarrow b$ （a是由b引起的）</li><li>If $a$ is the sending of a message in a process and $b$ is the receipt of the same message in another process, then $a \to b$.</li><li>If $a \to b$, an event $b \neq a$ <em>casually depends</em> on $a$.</li><li>If neither $a \to b$ nor $b \to a$ , written $a \parallel b$  , two events $a \neq b$  are <em>causally independent</em>, also called <em>concurrent</em>. （如果a和b没有因果关系，可以理解为他们是并发的）</li></ul><p>logical clock解决的是给分布式系统中所有时间定一个顺序，使这个顺序可以正确排列出有因果关系的事件，使得分布式系统在逻辑上不会发生因果倒置的错误。</p><p>We have several interpretations:</p><ul><li>$a \to b \Rightarrow$ $b$ casually depends on $a$</li><li>$a \parallel b \Rightarrow$ $a$ and $b$ have not influenced each other causally</li><li>$a \to b \Leftrightarrow$ as the time ascending, one can get from $a$ to $b$ in space-time diagram by following the process lines and message lines.</li></ul><p>Casual order: If the sending of message $m_2$ causally depends on sending the message $m_1$, then no receiver getting both messages delivers $m_2$ before $m_1$.</p><p>Total delivery order: If two processes $P$ and $Q$ both deliver the messages $m_1$ and $m_2$ , then $P$ delivers $m_1$ before $m_2$ only if $Q$ also does that.</p><p>Total FIFO order: both FIFO and total</p><p>Total causal order: both causal and total</p><h2 id="Logical-clocks"><a href="#Logical-clocks" class="headerlink" title="Logical clocks"></a>Logical clocks</h2><p>For each event $e$, assign a time stamp $C(e)$</p><p>Each process $P_i$ manages a counter $C_i$, when a event $e$ occurs, $C_i$ increases 1, the event gets the new value as <em>logical time stamp</em>.</p><p>$C(e)$ defines a partial order on the set of events, $e_1 &lt; e_2 \Leftrightarrow C(e_1)&lt;C(e_2)$ </p><p>The time stamp $C^ {‘} (e_i)$ of an event $e_i$ is a pair $(C_i, P_i)$, $e_1 &lt; e_2 \Leftrightarrow C^{‘}(e_1) &lt; C^{‘}(e_2) \Leftrightarrow C_1&lt;C_2 \vee C_1 =C_2 \wedge P_1 &lt; P_2$ </p><p>For two arbitrary events, $e1 \neq e_2 \Rightarrow e_1 &lt; e_2 \vee e_2 &lt; e_1$ </p><p><em>But the simple logical time does not consider the causal correlation between events.</em></p><h3 id="Clock-condition-Lamport"><a href="#Clock-condition-Lamport" class="headerlink" title="Clock condition (Lamport)"></a>Clock condition (Lamport)</h3><p>For all events $a, b$ , apply: $a \rightarrow b \Rightarrow C(a) &lt; C(b)$, so that the clock preserves the causal order of the events.</p><p>Only applies $C(a) &lt; C(b) \Rightarrow a \rightarrow b \vee a \parallel b$</p><p>also $C(a) = C(b) \Rightarrow a \parallel b$</p><p>$a \rightarrow b$ 可以推出$C(a) &lt; C(b)$， 但反过来不一定</p><p>Logical Clock解决的问题是找到一种方法，给分布式系统中所有时间定一个序，这个序能够正确地排列出具有因果关系的事件(不能保证并发事件的真实顺序)</p><p>The logical time stamps define a partial order on the set of events that maintains the causal connection between events.</p><p>Lamport’s Clocks Realization:</p><p>Each $P_i$ has a logical clock $L_i$, whose value is adapted at the occurrence of the following events:</p><ul><li>local event with process $P_i$: $L_i$ ++, the event gets new value as time stamp.</li><li>$P_i$ sends a message: $L_i$ ++, the send event gets new value as time stamp</li><li>$P_i$ receives a message: $L_i = max(L_i, t_m)+1$ (前续事件的时间戳与接受到的消息的时间戳中较大者+1)</li></ul><p>Problem: We can not be sure whether two events causally depend on each other only by a time stamp. </p><h3 id="Vector-clocks-Mattern-Fidge"><a href="#Vector-clocks-Mattern-Fidge" class="headerlink" title="Vector clocks(Mattern, Fidge)"></a>Vector clocks(Mattern, Fidge)</h3><p>Each process $P_i$ holds a vector time stamp $V_i$ consisting of n counters that are initially all zero.</p><ul><li>If an event occurs in a process $P_i$, the i-th component of its vector increases 1.</li><li>If $P_i$ sends a message, the new version of $V_i$ is sent along.</li><li>If $P_i$ receives a message with vector time stamp $T$, it forms the maximum of the new version of $V_i$ and $T$, <em>component-by-component</em>.(v1, v2两个向量，如果v1中的每个元素都不比其在v2中对应的元素小，那么v1&gt;v2；若两个向量中都有元素比对方向量的对应元素大，那么v1=v2；max(v1,v2)为两个向量的最大元素组成的向量)</li></ul><h3 id="Matrix-clocks"><a href="#Matrix-clocks" class="headerlink" title="Matrix clocks"></a>Matrix clocks</h3><p>Every process $P_i$ has a matrix time stamp $M_i$ given by a $n \times n$ -matrix initialized by zero matrix.</p><p>If a local event occurs, process $P_i$ increases the components $M_i[i,i]$ (the i-th element at diagonal) by 1.</p><p>Interpretation of the elements of matrix $M_i$:</p><ul><li>$M_i[i,i]$ is the local clock $P_i$</li><li>$M_i[i,l]$ is the knowledge of $P_i$ on the local clock of $P_l$</li><li>$M_i[k,l]$ is the knowledge of $P_i$ on the knowledge of $P_k$ on the local clock of $P_l$ </li></ul><p>If $P_j$ gets a message from $P_i$,</p><ul><li>update the time vector of $P_j$ with the time vector of $P_i$: what $P_j$ knows about the local clock of $P_k$</li><li>update the knowledge of $P_j$ on the time vector of other processes $P_k$: what $P_j$ knows about what $P_k$ knows on the local clock $P_i$.</li></ul><h2 id="Application-of-vector-and-matrix-clocks"><a href="#Application-of-vector-and-matrix-clocks" class="headerlink" title="Application of vector and matrix clocks"></a>Application of vector and matrix clocks</h2><h3 id="application-of-vector-clocks-Causal-broadcast"><a href="#application-of-vector-clocks-Causal-broadcast" class="headerlink" title="application of vector clocks: Causal broadcast"></a>application of vector clocks: Causal broadcast</h3><p>each message shall be sent to all processes, and should satisfy causality.</p><p>$P_i$ only increments $V_i[i]$ if it sends a message</p><p>Delivery condition: A message sent by $P_i$ is only delivered to $P_j$ when the time stamp $T$ fulfills the condition: $T[i] = V_j[i]+1 \wedge \forall k \neq i: T[k] \le V_j[k]$ </p><p>It is sufficient to increase $V_j[i]$ by 1 at delivery, don’t need to calculate the maximum.</p><p>一个进程会给其他进程发消息，对同一个进程如果发送了两个消息i和j，那么其他所有进程应该先接到j再接到i，如果某个进程在收到j之前收到了i，会直到收到j再接收i。</p><h3 id="application-of-lamport-clocks-Causal-multicast-and-causal-unicast"><a href="#application-of-lamport-clocks-Causal-multicast-and-causal-unicast" class="headerlink" title="application of lamport clocks: Causal multicast and causal unicast"></a>application of lamport clocks: Causal multicast and causal unicast</h3><p>Lamport clock alone are not sufficient for implementing a causal multicast.</p><p>We can make processes send messages periodically or request messages on demand to ensure FIFO channels.</p><p>But this solution delays delivery.</p><p>It can also be used for causal unicast, but the delivery of messages can be unnecessarily delayed.</p><p>=&gt; matrix clocks for causal unicast.</p><h3 id="Application-of-matrix-clocks-Causal-unicast"><a href="#Application-of-matrix-clocks-Causal-unicast" class="headerlink" title="Application of matrix clocks: Causal unicast"></a>Application of matrix clocks: Causal unicast</h3><p>Every process $P_i$ has a matrix time stamp $M_i$ given by a $n \times n$ -matrix initialized by zero matrix.</p><p>If $P_i$ sends a message to $P_j$, $M_i[i,j]$ is incremented, and the new version of matrix $M_i$ is sent together with the message.</p><p>If $P_j$ gets a message, it calculates the component-wise maximum of its own matrix $M_j$ and the matrix $M$ contained in the message. $1 \le i, k\le n: M_j[i,k] = max(M_j[i,k], M[i,k])$ </p><p>在进程$P_i$给$P<em>j$发送信息时，$P</em>{ii}$ 加1，无论是$P_j$ 知道的$P_i$ 已发给$P_j$ 的消息数，还是其他进程认为的$P_j$已知的$P_i$发给$P<em>j$ 的消息数都不能大于$P</em>{ii}$</p><p>Delivered condition:</p><ul><li>$M[i,j]=M_j[i,j]+1$ to make sure the message is the next message that $P_j$ expects from $P_i$</li><li>$ \forall k \neq i, 1 \le k \le n: M[k,j] \le M_j[k,j]$ </li></ul><p>The maximum of $M$ and $M_j$ is calculated after delivery of the message</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;• Why do we need clocks in distributed systems?&lt;br&gt;• What’s the problem anyway, ever computer in the system has a clock…&lt;br&gt;• Synchronization of physical clocks, how do we realize this?&lt;br&gt;• What are Logical Clocks, and what kind of logical clocks do you know?&lt;/p&gt;
&lt;p&gt;Time in Distributed Systems&lt;/p&gt;
&lt;p&gt;Synchronization of physical clocks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;External clock synchronization after Cristian&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Order of events&lt;/p&gt;
&lt;p&gt;Logical clocks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lamport clocks&lt;/li&gt;
&lt;li&gt;vector clocks&lt;/li&gt;
&lt;li&gt;application of vector clocks: causal broadcast&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="Distributed Algorithm" scheme="http://yoursite.com/tags/Distributed-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Distributed Algorithms C5 Mutual Exclusion</title>
    <link href="http://yoursite.com/2017/03/04/DAlg-notes-C5/"/>
    <id>http://yoursite.com/2017/03/04/DAlg-notes-C5/</id>
    <published>2017-03-04T08:34:15.000Z</published>
    <updated>2017-03-08T08:39:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>• We discussed several types of algorithms for mutual exclusion, can you tell me which?<br>• What are the two main requirements for mutual exclusion algorithms?<br>• There is often another requirement needed, which is…?<br>• Explain the … algorithm.</p><p>problem of mutual exclusion</p><p>Algorithm with central coordinator</p><p>Broadcast-based algorithms</p><p>Quorum-based algorithms</p><p>Token-based algorithms</p><p>Comparison of algorithms</p><a id="more"></a><h2 id="Mutual-exclusion"><a href="#Mutual-exclusion" class="headerlink" title="Mutual exclusion"></a>Mutual exclusion</h2><p>Coordination of the exclusive access on resources, often, there’s only 1 process shall access the resource</p><p>If a process has the right to access, he releases it after finite time voluntarily.</p><h3 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h3><ul><li><p>safety: something bad that cannot be undone shall never happen, 不该发生的绝不发生</p></li><li><p>liveness: something that should happen eventually happens，该发生的确保发生</p></li><li><p>fairness: No starvation, if a process wants access, the access has to be allowed after finite time.</p><p>strong fairness: The allowance of access takes the order of access requests into account</p></li><li><p>often a trivial solution is possible for only one of the safety and liveness.</p></li></ul><h2 id="Algorithm-with-Central-Coordinator"><a href="#Algorithm-with-Central-Coordinator" class="headerlink" title="Algorithm with Central Coordinator"></a>Algorithm with Central Coordinator</h2><p>A process is assigned as <em>coordinator</em>.</p><p>Coordinator grants accesses, 3 messages per access with blocking operations.</p><p>e.g: suppose processes 1, 2, 3 and a coordinator k. If process 1 wants to access, it should ask k if the resource is free, if k answers free, it gets access and send a message to tell k it has done the job.</p><p>Disadvantages:</p><ul><li>Single point of failure</li><li>Asymmetrical load distribution</li></ul><h2 id="Broadcast-Based-Algorithms-Lamport"><a href="#Broadcast-Based-Algorithms-Lamport" class="headerlink" title="Broadcast-Based Algorithms (Lamport)"></a>Broadcast-Based Algorithms (Lamport)</h2><p>Assume that all messages have an unique <em>logical time stamps</em>, lossless FIFO-communication channels.</p><h3 id="Basic-idea"><a href="#Basic-idea" class="headerlink" title="Basic idea:"></a>Basic idea:</h3><ul><li>Each process manages a message queue ordered according to time stamps.</li><li>Requests and releases are sent to all processes via broadcast.</li></ul><p>a process must only access if:</p><ul><li>its own request is the first request in its own queue</li><li>It already received a message (request 和 confirmation都可以) from each other process with a larger time stamp</li></ul><h3 id="Broadcast-Algorithm"><a href="#Broadcast-Algorithm" class="headerlink" title="Broadcast Algorithm:"></a>Broadcast Algorithm:</h3><p>每个进程维护一个队列，请求开始时，先将请求插入自身队列，发送request给其他进程，其他进程将请求插入自己的队列，再回确认消息，执行任务后发送释放资源的消息给其他进程，再将这个请求移出队列。</p><ol><li>Issue access request: insert request into own queue, send it to all other processes</li><li>Receive access request: insert request into its own queue according to the order of time stamp, send request confirmation to requesting process.</li><li>Send release after access: remove own request from own queue, send release to all other processes.</li><li>Received release: remove request from own queue.</li></ol><p>message complexity: sending $n-1$ requests, $n-1$ processes send their confirmation, sending of release to $n-1$ processes. Thus, $3(n-1)$ messages per access altogether.</p><h3 id="Improvement-by-Ricart-and-Agrawala"><a href="#Improvement-by-Ricart-and-Agrawala" class="headerlink" title="Improvement (by Ricart and Agrawala):"></a>Improvement (by Ricart and Agrawala):</h3><p>Idea: avoid explicit release messages through delayed confirmation. (不要release的消息，用delayed confirmation代替，以至每个access只需要$2(n-1)$条消息)</p><p>Issue access request: For a new request, give a sequence number to this request, this number is by 1 larger than all previously received requests, then send request to all other $n-1$ processes, after receive $n-1$ confirmations, access.</p><p>When a request arrives: if not applied or the sender has “older rights”, send confirmation immediately, other wise, confirmation is sent only after the ending of the own access.</p><p>一个进程想访问资源，它发送一个消息包含request，进程号和序列号（从1开始，大于该进程之前收到的请求）给其它进程告诉大家它要访问资源。对于接收者，如果它已获得对资源的访问，它就不进行应答，直到完成它的访问再发送delayed confirmation给发送者；如果是接收者想访问资源但还没开始，它会比较自身请求的序列号和它收到的请求的序列号，序列号小的胜出（序列号也可以是时间戳）；如果接收者没有访问资源也不打算访问，就发一个immediate confirmation给发送者。</p><h3 id="Requirements-1"><a href="#Requirements-1" class="headerlink" title="Requirements:"></a>Requirements:</h3><p>a solution that requires less message per access and still distributes the load equally between all processes.</p><p>a solution which does not include the involvement of all process in each coordination and still distributes the load equally between all processes.</p><h2 id="Quorum-Based-Algorithms"><a href="#Quorum-Based-Algorithms" class="headerlink" title="Quorum Based Algorithms"></a>Quorum Based Algorithms</h2><h3 id="Process-mesh-algorithm-Maekawa"><a href="#Process-mesh-algorithm-Maekawa" class="headerlink" title="Process mesh-algorithm (Maekawa)"></a>Process mesh-algorithm (Maekawa)</h3><p>基于子集请求而不是全集请求的互斥算法</p><p>$n$ processes are arranged in a quadratic mesh with an edge length of $\sqrt{n}$</p><p>A process $P_i$ must ask its granting set $R_i$ for allowance before access. For all pairs of process $P_i$ and $P_j$, their $R_i$ and $R_j$ have at least two processes in common. Granting sets have the cardinal number $2\sqrt{n}-2$, namely $R_i - P_i$ </p><p>Message complexity:</p><ul><li><p>send request to $2\sqrt{n}-2$ processes</p></li><li><p>$2\sqrt{n}-2$ processes send confirmation</p></li><li><p>send release to  $2\sqrt{n}-2$ processes</p><p>==&gt; $3[2\sqrt{n}-2]$ messages per access altogether.</p></li></ul><p>Problem: deadlocks may occur. </p><ol><li>introduce two additional message types to avoid deadlocks.</li><li>increase the number of messages per access on $5[2\sqrt{n}-2]$ in the worst-case.</li></ol><p>Another arrangement involving smaller cardinal number of the granting set?</p><p>=&gt; triangular arrangement</p><p>Use triangular arrangement, a single common process would be enough, with granting set size about $\sqrt{2}\sqrt{n}$ .</p><p>Problem: some processes may need more confirmation.</p><p><strong>???solution for load balancing???</strong></p><p>used for 2 different schemes</p><p>The first is nodes on same column and the one on the top left, the second is nodes on same row and the one down on the right side.</p><p>For the same scheme, granting set intersects with each other.</p><p>Also, granting set can intersect with granting set of other schemes.</p><p>All processes occur altogether in both schemes equally often in a granting set.</p><p>=&gt; <em>Thus, use both schemes randomly can attain load balancing.</em></p><h3 id="Minimal-Arrangement"><a href="#Minimal-Arrangement" class="headerlink" title="Minimal Arrangement"></a>Minimal Arrangement</h3><p>Suppose $K$ is the size of the granting set, then a minimal arrangement exists if $K-1=p^m$, $p$ is a prime number, $n$ is a natural number, then the arrangement has $n=K(K-1)+1$ processes.</p><h2 id="Token-Based-Algorithms"><a href="#Token-Based-Algorithms" class="headerlink" title="Token Based Algorithms"></a>Token Based Algorithms</h2><h3 id="Simple-token-Ring-solution-Le-Lann"><a href="#Simple-token-Ring-solution-Le-Lann" class="headerlink" title="Simple token Ring-solution (Le Lann)"></a>Simple token Ring-solution (Le Lann)</h3><ul><li>Processes are arranged in a logical ring</li><li>Access is controlled by circulation token</li><li>Applicants waits for access until token reaches it</li><li>Accessing process relays the token with the release</li><li>Process without access intention relays the token directly</li><li>possible to use separate tokens for coordinating access to individual resources</li></ul><p>进程0得到token后沿着环进行传递，进程获得令牌时，如果有访问需求，就对资源进行访问，完成后继续向后传递token，如果没有则直接传token给后继。</p><p>advantages: simple, correct, fair, no deadlocks, no starvation.</p><p>Disadvantages: token is always on the way, sometimes it’s useless. The message number per request is not limited. Need to wait for a long time if there’re large number of processes.</p><h3 id="Token-based-solution-Suzuki-and-Kasami"><a href="#Token-based-solution-Suzuki-and-Kasami" class="headerlink" title="Token-based solution (Suzuki and Kasami)???"></a>Token-based solution (Suzuki and Kasami)???</h3><p>A requesting process sends a request with its sequence number to all other processes (ring circuit or via broadcast)</p><p>Each process $P_i$ stores the highest currently received sequence number in a list $R_i$.</p><h3 id="Lift-Algorithm-Raymond"><a href="#Lift-Algorithm-Raymond" class="headerlink" title="Lift Algorithm (Raymond)"></a>Lift Algorithm (Raymond)</h3><p>Use a spanning tree for selective relay (转发) of the request in direction to the token.</p><p>The edges have two directions.</p><p>The token wanders against the arrow direction, every time the token passed an edge, it changes the direction of this edge.</p><p>A process that wants the token sends request over its outgoing edge.</p><p>request顺着箭头方向发，token逆着箭头方向发，token经过一条边这条边的方向会被改变。</p><p>每个process记录给它发送request的process</p><p>If a process receives the token, it can send it in one of the requesting directions, or if there are more requests from other directions, it sends a request after the token.</p><p>$O(log_kn)$ messages per access are needed, $k$ is the k-ray.</p><blockquote><p>a k-ary tree is a rooted tree in which each node has no more than k children</p></blockquote><p>Start state: winner of an election gets the token and creates a spanning tree with edges directed towards itself.</p><h2 id="Comparison-of-message-complexity-per-access"><a href="#Comparison-of-message-complexity-per-access" class="headerlink" title="Comparison of message complexity per access"></a>Comparison of message complexity per access</h2><table><thead><tr><th>procedure</th><th>message complexity</th></tr></thead><tbody><tr><td>Token Ring</td><td></td></tr><tr><td>Simple broadcast</td><td></td></tr><tr><td>Improved broadcast</td><td></td></tr><tr><td>improved token ring</td><td></td></tr><tr><td>mesh arrangement</td><td></td></tr><tr><td>lift algorithm on k-ary tree</td><td></td></tr><tr><td>central manager</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;• We discussed several types of algorithms for mutual exclusion, can you tell me which?&lt;br&gt;• What are the two main requirements for mutual exclusion algorithms?&lt;br&gt;• There is often another requirement needed, which is…?&lt;br&gt;• Explain the … algorithm.&lt;/p&gt;
&lt;p&gt;problem of mutual exclusion&lt;/p&gt;
&lt;p&gt;Algorithm with central coordinator&lt;/p&gt;
&lt;p&gt;Broadcast-based algorithms&lt;/p&gt;
&lt;p&gt;Quorum-based algorithms&lt;/p&gt;
&lt;p&gt;Token-based algorithms&lt;/p&gt;
&lt;p&gt;Comparison of algorithms&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="Distributed Algorithm" scheme="http://yoursite.com/tags/Distributed-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Distributed Algorithm C4 -- Election Algorithms</title>
    <link href="http://yoursite.com/2017/03/03/DAlg-notes-C4/"/>
    <id>http://yoursite.com/2017/03/03/DAlg-notes-C4/</id>
    <published>2017-03-03T08:34:12.000Z</published>
    <updated>2017-03-08T08:38:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>• Why do we need election algorithms?<br>• Which election algorithms do you know?<br>• Explain the … algorithm.<br>• What are the drawbacks of this algorithm and can it be improved?<br>• What is the message complexity of this algorithm.</p><p>Election algorithms for</p><ul><li>arbitrary connected topologies</li><li>unidirectional and bidirectional rings</li><li>trees</li></ul><p>Randomized election algorithms for</p><ul><li>bidirectional rings</li><li>anonymous rings</li></ul><a id="more"></a><h2 id="The-election-problem"><a href="#The-election-problem" class="headerlink" title="The election problem"></a>The election problem</h2><p>Select a <em>unique</em> leader from a set of identical processes, assume that each node has a unique integer identity &gt; 0.</p><p>It requires each node shall know the winner in the end, also everyone can initiate the algorithm. For example, determine the largest identity in the topology can be used as election algorithm.</p><h2 id="Election-for-arbitrary-topologies"><a href="#Election-for-arbitrary-topologies" class="headerlink" title="Election for arbitrary topologies"></a>Election for arbitrary topologies</h2><p>We suppose that each process has a unique identity $p$ and a local variable $M_p$ , which is 0 initially.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">Ip: &#123;Mp == 0&#125;</div><div class="line">Mp = p</div><div class="line">SEND &lt;Mp&gt; TO all neighbors</div><div class="line">Rp: &#123;A message &lt;j&gt; has arrived&#125;</div><div class="line">IF Mp &lt; j, THEN:</div><div class="line">Mp = j;  </div><div class="line">SEND &lt;Mp&gt; to all other neighbors</div><div class="line">Tp: &#123;termination was discovered&#125;</div><div class="line">IF Mp == p, THEN:</div><div class="line">&quot;I am the master&quot;</div><div class="line">ELSE:</div><div class="line">&quot;Mp is the master&quot;</div><div class="line">FI</div></pre></td></tr></table></figure><p><strong>???Mp = p for every node, or each node initiates this algorithm at the same time???</strong></p><h3 id="Echo-election-algorithm"><a href="#Echo-election-algorithm" class="headerlink" title="Echo election algorithm"></a>Echo election algorithm</h3><p>Echo election algorithm is suitable for arbitrary connected topologies. </p><p>Each initiators starts an instance of this algorithm, the explorer and echo carry the identity of the initiators, weaker messages are not passed (message extinction), strongest wave prevails and terminates at the winner, so the winner know it has won. If a initiator receives a stronger message, it knows that it has lost the election, stronger initiators will not send echo for it.</p><p>The winner starts the echo algorithm again to tell other it has won, and other waves finally stagnate somewhere.</p><h3 id="Election-Algorithms-for-unidirectional-Rings"><a href="#Election-Algorithms-for-unidirectional-Rings" class="headerlink" title="Election Algorithms for unidirectional Rings"></a>Election Algorithms for unidirectional Rings</h3><h4 id="Bully-algorithm"><a href="#Bully-algorithm" class="headerlink" title="Bully- algorithm"></a>Bully- algorithm</h4><p>Each process wakes up as an initiator or receives a message from its neighbor. </p><p>Each waking up starts a complete ring circulation.</p><p>Finally, each node receives a message with its own identity and the largest node’s ID.</p><p>If both IDs are same, namely in the final message of each nodes $<i, j="">$, $i = j$, then the node is the master, otherwise it has lost.</i,></p><p>For every circulation of each process:</p><ol><li><p>Each node holds its identity value $p$. The initiator sends a message like $<i, j="">$ to its next node, $i$ is its identity value, $j$ is the highest value currently.</i,></p></li><li><p>When a node receives a message:</p><p>firstly it compares $i$ and its $p$ to check if a cycle of message exchange has finished. </p><ul><li>If not, then it compares the current highest value in the message $j$ to its identity value $p$, find the largest between $j$ and $p$, $k = max(j, p)$, and send message $<i, k="">$ to the next node.</i,></li><li>Else, this node compares the current highest value in the message $j$ to its own value $p$, if they are equal, then this node is the master, else, the node hold the value $j$ is the master.</li></ul></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">Ip: &#123;init == FALSE&#125;</div><div class="line">init = TRUE;</div><div class="line">SEND &lt;p, p&gt; TO next node.</div><div class="line">    # the first p is the sender ID, and the second p is highest ID known</div><div class="line">Rp: &#123;A message &lt;i, j&gt; has arrived&#125;</div><div class="line">IF i != p, THEN:</div><div class="line">k = max(j, p);</div><div class="line">SEND &lt;i, k&gt; TO next node;</div><div class="line">ELSE:</div><div class="line">IF p == j, THEN:</div><div class="line">&apos;I am the master&apos;</div><div class="line">ELSE:</div><div class="line">&apos;j is the master&apos;</div><div class="line">FI</div><div class="line">FI</div></pre></td></tr></table></figure><h5 id="complexity"><a href="#complexity" class="headerlink" title="complexity"></a>complexity</h5><p>We need $n$ complete circulations.</p><p>$n^2$ single messages totally. The message complexity is $O(n^2)$.</p><h5 id="drawbacks-and-improvements"><a href="#drawbacks-and-improvements" class="headerlink" title="drawbacks and improvements"></a>drawbacks and improvements</h5><p>Only winners know it has won, we can add an extra round to inform all other nodes.</p><p>In this method, messages cannot lead to a win are passed on.</p><p>=&gt; We can use message extinction to fix these two problems.</p><h4 id="message-extinction-Chang-and-Roberts"><a href="#message-extinction-Chang-and-Roberts" class="headerlink" title="message extinction (Chang and Roberts)"></a>message extinction (Chang and Roberts)</h4><p>every node initiates a ring circuit, send its value to the next node, if its value larger than the next node’s, pass the value to the next after that, else this message will be swallowed. If a node can receive a message includes a value equal to its own value, that means this node is the master.</p><h5 id="features"><a href="#features" class="headerlink" title="features"></a>features</h5><ul><li>messages are only passed on if they can lead to a win, all other messages are extinct</li><li>only the winner receives its own message, use an additional ring circulation to inform other nodes that it won.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Ip: &#123;Mp == 0&#125;</div><div class="line">Mp = p;</div><div class="line">SEND &lt;Mp&gt; TO next node;</div><div class="line">Rp: &#123;message &lt;j&gt; has arrived&#125;</div><div class="line">IF Mp &lt; j, THEN:</div><div class="line">Mp = j;</div><div class="line">SEND &lt;Mp&gt; TO next node;</div><div class="line">FI</div><div class="line">IF j == p, THEN:</div><div class="line">&apos;I am the master&apos;</div><div class="line">&lt;inform others by another ring circuit&gt;</div><div class="line">FI</div></pre></td></tr></table></figure><h5 id="Message-complexity"><a href="#Message-complexity" class="headerlink" title="Message complexity"></a>Message complexity</h5><ol><li><p>worst case message complexity with k initiators happens when initiators are arranged on the ring in descending order and initiate election in ascending order. (消息传递的方向与节点值增大的方向相反)</p><p>suppose we have n nodes:</p><p>​            k-largest initiator, $n-(k-1)$ messages;</p><p>​            …</p><p>​            2nd largest initiator, $n-1$ messages;</p><p>​            1st largest initiator, $n$ messages;</p><p>​    Thus, the message volume with k initiators in worst case is:</p><p>​          $ n + (n-1) + (n-2) + (n-3) + … + (n-(k-1))$</p><p>​$= \frac{n(n+1)}{2} - \frac{(n-k)(n-k+1)}{2} $</p><p>​$= nk - \frac{k(k-1)}{2}$</p><p>​==&gt; $O(n^2)$ with ring size $n$ and $k=n$  plus $n$ additional messages for win notification</p></li></ol><ol><li>for the best case, if the initiators are arranged on the ring in ascending order and initiate the election approximately simultaneously, except the largest initiator need $n$ messages, others only need 1 message. Thus the message volume is $ n+k-1$ plus $n$ additional messages. The message complexity is $O(n)$.</li><li>Average case. For i-largest initiator, needs $n/i$ messages on average. So the average-case message complexity is $nH_k \approx n lnk$ , with $H_k = 1+1/2+…+1/k$ , still need $n$ messages for winner notification.</li></ol><h5 id="Influence-of-message-overtaking"><a href="#Influence-of-message-overtaking" class="headerlink" title="Influence of message overtaking"></a>Influence of message overtaking</h5><h2 id="Election-algorithms-for-Bidirectional-rings"><a href="#Election-algorithms-for-Bidirectional-rings" class="headerlink" title="Election algorithms for Bidirectional rings"></a>Election algorithms for Bidirectional rings</h2><h3 id="Hirschberg-Sinclair-Election-Algorithm"><a href="#Hirschberg-Sinclair-Election-Algorithm" class="headerlink" title="Hirschberg-Sinclair-Election Algorithm"></a>Hirschberg-Sinclair-Election Algorithm</h3><p>In this algorithm, each node try to compare its value to its right $2^{i-1}​$ neighbors and left $2^{i-1}​$ neighbors ($i = 1,2,3…​$, left and right neighbors totally $2^i​$) on a bidirectional ring until message arrives again at the node; node survived if it is larger than both two sides neighbors, and it receives an OK if it is larger than one neighbor, if node receive 2 OK in one phase, then it survived, else its state turns to passive from active.</p><h4 id="worst-case-message-complexity"><a href="#worst-case-message-complexity" class="headerlink" title="worst case message complexity"></a>worst case message complexity</h4><p>Suppose we have n nodes in a ring:</p><p>​    After phase 1, 1 passive process between active processes, max. $\frac{n}{2}$ nodes survive</p><p>​    After phase 2, 2 passive between active, max. $\frac{n}{3}$ survive</p><p>​    After phase 3, 4 passive between active, max. $\frac{n}{5}$ survive</p><p>=&gt; there will be $2^{i-1}$ passive processes between active processes after phase $i$, max. $\frac{n}{1+2^{i-1}}$  nodes survive.</p><p>=&gt;<strong>??? $\frac{n}{1+2^{i-1}}$ processes(nodes) can initiate chains with the length $2i$ </strong></p><p>Each chain with the length $2i$ generates at most $4*2i$ messages</p><p>in phase i, there are max $\frac{4 \times 2^i \times n}{1+2{i-1}} = \frac{2^{i-1} \ times 8n}{1+2^{i-1}} &lt; 8n$ messages.</p><p>There are maximal $1+log2n$ phases, thus at the most $8n + 8nlog2n$ messages, which has the time complexity $O(nlogn)$, also need $n$ messages for win notification.</p><p><strong>???time complexity $4n-2$ for $n=2k$ (worst case) and $6n-6$ for $n=2k+1$ (best case)???</strong></p><h4 id="drawbacks-and-improvements-1"><a href="#drawbacks-and-improvements-1" class="headerlink" title="drawbacks and improvements"></a>drawbacks and improvements</h4><p>Drawback: The initiator do not have to proceed the phases synchronously, thus an initiator which is already in a high phase can still stopped by a new larger initiator.</p><p>Improvement: pairs are used and ordered lexicographically rather than by node identities only.</p><ul><li>an initiator in a higher phase always prevails against an initiator in a lower phase, only if the phase is the same the node identity can be a tie breaker.</li><li>the algorithm is no longer a MAX-algorithm, it does not necessarily determine the node with the highest id as the winner.</li></ul><h3 id="Peterson-Election-Algorithm"><a href="#Peterson-Election-Algorithm" class="headerlink" title="Peterson Election Algorithm"></a>Peterson Election Algorithm</h3><p>和HS算法的区别在哪？？？？</p><p>In the beginning, all processes are active.</p><p>For each phase:</p><ul><li>Each active process communicates its IDs to the next active process in both directions.</li><li>Process receives higher ID become passive and only pass messages on</li><li>Only active processes participate in the next phase</li></ul><p>A node wins when it receives its own ID.</p><p>Additionally, a circulation for the win notification with $n$ messages.</p><h4 id="worst-case-message-complexity-1"><a href="#worst-case-message-complexity-1" class="headerlink" title="worst-case message complexity"></a>worst-case message complexity</h4><p>If a node survives, its neighbors do not survive.</p><p>In each phase, the number of active processes is at least halved, which means there are maximal $log_2n + 1$ phases.</p><p>Each node sends at most $2$ messages per phase, so $2n$  messages per phase at most.</p><p>=&gt; Worst case message complexity: $2n(log_2n +1)$</p><h4 id="Best-case-message-complexity"><a href="#Best-case-message-complexity" class="headerlink" title="Best-case message complexity"></a>Best-case message complexity</h4><p>If each node has one neighbor whose ID is higher than its ID in the first phase, then there is at most 2 phases to terminate the algorithm with $2n$ messages each.</p><p>=&gt; At most $4n$ messages. </p><blockquote><p>????The message sent from the largest to the smallest node circulates the ring -&gt; n messages</p><p>????all other messages go only to their respective neighbor -&gt; 2n-1 messages</p><p>????The algorithm terminates after only one phase, msg complexity is $3n-1$</p></blockquote><h4 id="Average-case-message-complexity"><a href="#Average-case-message-complexity" class="headerlink" title="Average-case message complexity"></a>Average-case message complexity</h4><p>First, arrange all nodes from largest to smallest. Then for each node, calculate the probability of it is smaller than the left neighbor and right neighbor, multiply these two probability value to get the probability this node is smaller than its two neighbors. Next, calculate the mean value of these probability that a node is smaller than both-side neighbors.</p><p>=&gt; A node survives a phase with the probability 1/3, and  $(log_3n+1)$ phases on average.</p><p>=&gt; Average-case is $2n(log_3n +1)$</p><h3 id="Unidirectional-Peterson-Election"><a href="#Unidirectional-Peterson-Election" class="headerlink" title="Unidirectional Peterson Election???"></a>Unidirectional Peterson Election???</h3><p>An active node compares its value with the value of the next active predecessor and the value of the next active successor. If it has the largest ID, it remains active, otherwise it becomes passive. 往一个方向的后继的两个节点发送当前节点的id</p><p>But on unidirectional rings messages can only be sent forward.</p><p>Solution: </p><ul><li>The IDs of the active predecessor and the current node are transmitted to the active successor, stored in the variables $v$ and $p$.</li><li>The comparison of the values with the own IDs is carried out by the successor.</li><li>If $v&gt;max(p,s)$ , it remains active and joins the next phase. (从左到右三个节点，分别有值p, v, s， 如果v比p和s都大，那么这个节点存活并进入下一阶段 )</li></ul><h2 id="Election-Algorithms-on-Trees"><a href="#Election-Algorithms-on-Trees" class="headerlink" title="Election Algorithms on Trees"></a>Election Algorithms on Trees</h2><h3 id="Three-phases"><a href="#Three-phases" class="headerlink" title="Three phases:"></a>Three phases:</h3><ol><li>Explosion phase: Election request is propagated to the leafs.<ul><li>The explosion starts at several initiators</li><li>If a node receives a explosion message for the first time, it will pass it on to all other neighbors</li><li>The explosion waves unite when explosion messages meets on the same edge</li></ul></li><li>Contraction phase: from the leafs the maximum of the already collected identities is propagated to the center.<ul><li>If a leaf node receives a explosion message, it will answer with its own identity immediately</li><li>If it is not a leaf node, it will compare the identities it received and its own identity, and send the max one over the last remaining edge.</li><li>In the end, two different max identity will meet on the same edge, both received nodes know the real maximum afterwards</li></ul></li><li>Information phase: Distribution of the real maximum from the center to all nodes in the network.<ul><li>from both nodes the maximum is flooded into the network, the edge between them is omitted.</li></ul></li></ol><h3 id="message-complexity-with-k-initiators"><a href="#message-complexity-with-k-initiators" class="headerlink" title="message complexity with k initiators"></a>message complexity with k initiators</h3><p>We assume that there are $n$ edges and $k$ initiators.</p><ul><li>explosion phase: one message over each edge, except 2 messages over $k-1$ <strong>meeting edges(???)</strong>, so the number of messages in this phase is $n-2+k$.</li><li>contraction phase: one message over each edge, two messages over the central edge, in total $ (n-1)-1=n-2$ messages.</li><li>information phase: one message over each edge, but no message over the central edge, totally $n$ messages.</li></ul><p>Altogether $3n + k -4$ messages.</p><p>Obviously, Election on trees is more efficient that Election on rings.</p><h2 id="Randomized-Election-Algorithms"><a href="#Randomized-Election-Algorithms" class="headerlink" title="Randomized Election Algorithms"></a>Randomized Election Algorithms</h2><p>Randomized algorithms are often more simple than deterministic algorithms that solve the same problem. Some problems can be solved more efficiently via randomized algorithms.</p><p>There are two categories of randomized algorithms, respectively <em>Las Vegas-Algorithms</em> and <em>Monte Carlo-algorithms</em>. For Las Vegas-algorithms, they are weakening of the termination, provide correct result, but the worst-case time complexity is unlimited. While Monte Carlo-algorithms are weakening partial correctness, but the worst-case run time is limited. </p><p>(Las Vegas算法保证准确性但时间复杂度高，Monte Carlo算法保证效率但可能会出错)</p><h3 id="Randomized-Election-in-Bidirectional-Rings"><a href="#Randomized-Election-in-Bidirectional-Rings" class="headerlink" title="Randomized Election in Bidirectional Rings"></a>Randomized Election in Bidirectional Rings</h3><p>Here, the average-case message complexity for $k=n$ is about $0.71n ln \ n$ . About 30% better than deterministic algorithm for unidirectional rings by Chang and Roberts.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;• Why do we need election algorithms?&lt;br&gt;• Which election algorithms do you know?&lt;br&gt;• Explain the … algorithm.&lt;br&gt;• What are the drawbacks of this algorithm and can it be improved?&lt;br&gt;• What is the message complexity of this algorithm.&lt;/p&gt;
&lt;p&gt;Election algorithms for&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;arbitrary connected topologies&lt;/li&gt;
&lt;li&gt;unidirectional and bidirectional rings&lt;/li&gt;
&lt;li&gt;trees&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Randomized election algorithms for&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bidirectional rings&lt;/li&gt;
&lt;li&gt;anonymous rings&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="Distributed Algorithm" scheme="http://yoursite.com/tags/Distributed-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Distributed Algorithm C3 -- Flooding, Broadcast and Echo</title>
    <link href="http://yoursite.com/2017/03/02/DAlg-notes-C3/"/>
    <id>http://yoursite.com/2017/03/02/DAlg-notes-C3/</id>
    <published>2017-03-02T08:33:11.000Z</published>
    <updated>2017-03-08T08:37:36.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Explain the Echo/Broadcast/Flooding algorithm.</li><li>What are the differences?</li><li>What is the message complexity?</li></ul><p>Overview:</p><p>Flooding:distribute info, with/without confirmation, all nodes and edges</p><p>Echo:distribute info, all nodes and edges, selective confirmation, collect info, spanning tree</p><p>Broadcast:distribute, all nodes, without acknowledgement with special topologies</p><p>Multicast: distribute, specific group, with/without acknowledgement</p><a id="more"></a><h2 id="Flooding"><a href="#Flooding" class="headerlink" title="Flooding"></a>Flooding</h2><p>Precondition: connected topology.</p><p>Principle: <strong>Each node tells a new rumor that it got from one of its neighbors to all other neighbors, until all nodes are informed, already known rumors are ignored.</strong></p><p>Initially, <code>informed == FALSE</code> for all processes.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">I:  informed == FALSE</div><div class="line">    SEND &lt;INFO&gt; TO all neighbors</div><div class="line">    informed = TRUE;</div><div class="line">R:  &#123;A message &lt;info&gt; received&#125;</div><div class="line">IF NOT informed, then:</div><div class="line">SEND &lt;INFO&gt; TO all other neighbors;</div><div class="line">informed = TRUE;</div><div class="line">FI</div></pre></td></tr></table></figure><p><em>Are several competing initiators allowed???</em></p><h3 id="evaluation-of-flooding"><a href="#evaluation-of-flooding" class="headerlink" title="evaluation of flooding"></a>evaluation of flooding</h3><p>suppose $n$ nodes and $e$ edges.</p><ul><li><p>how many messages are sent?</p><p>1) each node sends message to all its incident edges: $2e$</p><p>2) except the node which sent message to itself: $-n$</p><p>3) except the initator: $+1$</p><p>Thus, $2e-n+1$ messages are sent.</p></li><li><p>how to determine the termination? </p><p>=&gt; Confirmation</p></li></ul><h3 id="flooding-with-confirmation"><a href="#flooding-with-confirmation" class="headerlink" title="flooding with confirmation"></a>flooding with confirmation</h3><p>Two types of messages: <em>Explorers</em>, <em>confirmations</em></p><p>A process acknowledges an explorer with a confirmation, when it receives all confirmations for the explorers it sent.</p><p>If this process received the explorer for the first time, it sent confirmation after arrival of #neighbor -1 receipts, if it is leaf or received further explorer, sent confirmation immediately.</p><p>If the initiator received a confirmation from every neighbor, algorithm terminates.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">I:  &#123;Not informed&#125; // executed by the initiator</div><div class="line">SEND &lt;Explorer&gt; TO all neighbors;</div><div class="line">informed = TRUE;</div><div class="line">R:  &#123;explorer from neighbor N is received&#125;</div><div class="line">IF NOT informed, then:</div><div class="line">SEND &lt;Explorer&gt; TO all neighbors except N;</div><div class="line">informed = TRUE;</div><div class="line">A = N;</div><div class="line">ELSE:</div><div class="line">SEND Confirmation to N;</div><div class="line">FI</div><div class="line"></div><div class="line">&#123;Confirmation is received&#125;</div><div class="line">Count = Count + 1;</div><div class="line">IF (NOT Initiator) &amp;&amp; (Count == #Neighbors -1), then:</div><div class="line">SEND Confirmation TO Neighbor A;</div><div class="line">FI</div><div class="line">IF Initiator &amp;&amp; (Count == #Neighbors), then:</div><div class="line">Exit;</div><div class="line">FI</div></pre></td></tr></table></figure><h3 id="evaluation-of-flooding-with-confirmation"><a href="#evaluation-of-flooding-with-confirmation" class="headerlink" title="evaluation of flooding with confirmation"></a>evaluation of flooding with confirmation</h3><ul><li><p>how many explorers altogether?</p><p>every node sends explorers to its neighbors: $2e$ explorers</p><p>not include its activation edge: $-n$ explorers</p><p>except the initiator: $+1$ explorer</p><p>total: $2e - n + 1$ explorer</p></li><li><p>how many confirmations altogether?</p><p>$2e - n + 1$ confirmations</p></li><li><p>how many messages altogether?</p><p>$4e - 2n + 2$ messages</p></li></ul><h2 id="Echo"><a href="#Echo" class="headerlink" title="Echo"></a>Echo</h2><p>Begin with the initiator, it sends explorers to all its neighbors. If a node received the explorer for the first time, it records the activation edge and sends explorers to all its neighbors. If two explorers meet on an edge, the explorers are swallowed. </p><p>For a node have received an explorer before, if it receives an explorer or echo over all its edges, then it sends echo over its activation edge, leaf nodes immediately send echo when receive an explorer.</p><p>The algorithm terminates when the initiator receive echo.</p><p>Exactly two messages run over every edge.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">I:  &#123;NOT informed&#125; // executed by the initiator</div><div class="line">SEND &lt;EXPLORER&gt; to all its neighbors</div><div class="line">informed = True;</div><div class="line">R:  &#123;a message from neighbor N is received&#125;</div><div class="line">IF NOT informed, then:</div><div class="line">SEND &lt;Explorer&gt; to all other neighbors except N;</div><div class="line">informed = TRUE;</div><div class="line">A = N;</div><div class="line">FI</div><div class="line">count = count + 1;</div><div class="line">IF count == #neighbors, then:</div><div class="line">IF NOT Initiator:</div><div class="line">SEND &lt;ECHO&gt; TO neighbor A;</div><div class="line">ELSE:</div><div class="line">EXIT;</div><div class="line">FI</div><div class="line">FI</div></pre></td></tr></table></figure><h3 id="Evaluation-of-echo"><a href="#Evaluation-of-echo" class="headerlink" title="Evaluation of echo"></a>Evaluation of echo</h3><p>every node sends an explorer on all edges -&gt; $2e$ explorer</p><p>exception activation edge -&gt; $-n$ explorer</p><p>except the initiator -&gt; $+1$ explorer</p><p>every node sends an echo on the activation edge -&gt; $+n$ echos</p><p>except the initiator -&gt; $-1$ echo</p><p>totally, $2e$ messages.</p><h3 id="Characteristics-and-Difference-with-flooding"><a href="#Characteristics-and-Difference-with-flooding" class="headerlink" title="Characteristics and Difference with flooding"></a>Characteristics and Difference with flooding</h3><p>Echo is a <em>wave algorithm</em></p><ul><li>Distribution of information (to all nodes over all edges)</li><li>Collecting of information (of potentially all nodes over the activation edges)</li></ul><p>与flooding with confirmation相比，对于二者的节点来说，如果都是第一次收到explorer，会将explorer发给所有其他邻居；在confirm阶段，echo算法中节点只会将确认消息echo回给第一个给它发explorer的节点，而在flooding算法中，节点会发送confirmation给每一个给它发过explorer的节点以确认它收到了explorer。采用echo算法可以少发一些message。</p><p>Echo-edges form a spanning tree, the spanning tree looks different due to the message delays</p><h3 id="improvement-of-Echo-algorithm"><a href="#improvement-of-Echo-algorithm" class="headerlink" title="improvement of Echo algorithm"></a>improvement of Echo algorithm</h3><p>Idea: avoid visit nodes which are known to be visited by other explorers. 减少echo算法中发送的explorer数量。</p><p>implement: Together with an explorer, a set of taboo nodes $z$ is sent and received.</p><p>$z = <neighbors of="" initators=""> \cup <initiator>$</initiator></neighbors></p><p>Explorers only sent to neighbors set $y$ which is not included in z</p><p>Thus, the new taboo nodes set $z’ = z \cup y$</p><h2 id="Broadcast"><a href="#Broadcast" class="headerlink" title="Broadcast"></a>Broadcast</h2><p>sending of a message to all nodes, optionally with confirmation.</p><p>flooding algorithm is especially fault-tolerant because all edges are used for distribution of information, it is suitable for any connected undirected topology.</p><p>However, when focus on some specific topology, we don’t need to use all edges to send information, a broadcast with less messages is possible. For aimed case, each node is only reached over a single edge, only $n-1$ messages.</p><ul><li>Broadcast on unidirectional rings: all nodes are informed when the initiator receive the token again, $n$ messages.</li><li>Broadcast on Trees: Tree has $n-1$ edges (spanning tree?)</li><li>Broad cast on Hypercubes: time complexity – $d$ (#dimension) cycles; Message complexity: $n-1$</li></ul><h2 id="Multicast"><a href="#Multicast" class="headerlink" title="Multicast"></a>Multicast</h2><p>pairwise exchange messages is not optimal if communications are between a sender and many receivers.</p><p><em>Multicast operatioin</em> is to send a single message from one process to each member of a group processes (if send know its members who are in same group…)</p><p>Why multicast? </p><ul><li>Fault tolerance: client requests are multicast to a group of identical, replicated servers, even a server failed, the client still can be served.</li><li>Service discovery: use multicast we can discover services</li><li>Better performance through replicated data: multicast messages can be used to propagate changes to all copies.</li><li>event notifications: notify a group of processes when a new event happened.</li></ul><h3 id="Reliable-IP-Multicast"><a href="#Reliable-IP-Multicast" class="headerlink" title="Reliable IP Multicast"></a>Reliable IP Multicast</h3><p>can be implemented using <em>piggybacked acknowledgements and negative acknolwedgements</em></p><ul><li>piggyback ack: ack of delivery is attached to other message</li><li>negative ack: ack notify other process that a message is missing</li><li>use “deliver” instead of “receive”</li></ul><p>$S^p_g$ is denoted as the sequence number of process $p$ in group $g$, we can take it as how many messages this process multicast.</p><p>$R^q_g​$ is denoted as the sequence number(the number of messages) that current process delivered(received) from process $q​$ in group $g​$.</p><p>for a specific process $p1, p2, p3$ in group $g$, their $S^p_g$ and $R^q_g$ are all set to 0 initially.</p><ol><li>if $p1$ multicast a message, $S^{p1}_g$ will add 1.</li><li>then $p1, p2, p3$ received the message multicast from $p1$, $p2$’s $R^{p1}_g$ and $p3$’s $R^{p1}_g$ will both add 1.</li><li>After that, if $p2$ wants to multicast a message $m$, it piggybacks its sequence number $S^{p2}_g$ and a set of acknowledgements of the form $<q, r^q_g="">$, like $ m = { payload, 0, <1, 1=""> } $ </1,></q,></li><li>$p3$ and $p1$ received new message from $p2$, they will compare the piggybacked sequence number with their own $R^{p2}_g$ </li><li>take $p3$ as example, if $p3$’s $R^{p2}_g $ equals to the sequence number $S^{p2}_g$ of $p2$, the message will be delivered; if $S^{p2}_g \le R^{p2}_g$ (p2发出的比p3收到的少), the message has been seen before and is discarded; if $S^{p2}_g \ge R^{p2}_g$ (p2发出的比p3收到的多，可能有消息丢失), the message is stored in a <em>hold-back queue</em> and missing message are requested before delivery, then a negative ack is sent to either the sender or any other process which have delivered the missing message.<br>​</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Explain the Echo/Broadcast/Flooding algorithm.&lt;/li&gt;
&lt;li&gt;What are the differences?&lt;/li&gt;
&lt;li&gt;What is the message complexity?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overview:&lt;/p&gt;
&lt;p&gt;Flooding:distribute info, with/without confirmation, all nodes and edges&lt;/p&gt;
&lt;p&gt;Echo:distribute info, all nodes and edges, selective confirmation, collect info, spanning tree&lt;/p&gt;
&lt;p&gt;Broadcast:distribute, all nodes, without acknowledgement with special topologies&lt;/p&gt;
&lt;p&gt;Multicast: distribute, specific group, with/without acknowledgement&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="Distributed Algorithm" scheme="http://yoursite.com/tags/Distributed-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>HSI review 2 -- sample question and answer</title>
    <link href="http://yoursite.com/2017/02/12/HSIreview2/"/>
    <id>http://yoursite.com/2017/02/12/HSIreview2/</id>
    <published>2017-02-12T07:50:12.000Z</published>
    <updated>2017-02-13T23:16:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>some sample questions from spoiler.</p><a id="more"></a><h2 id="Creativity"><a href="#Creativity" class="headerlink" title="Creativity"></a>Creativity</h2><p>sample question</p><p><img src="/img/选区_020.png" alt="选区_020"></p><ol><li>workplace culture encourages innovation of employees. That’s an extrinsic innovation fluence. People who work in a environment which gives them freedom and competence will make them more active in innovation.</li></ol><ol><li><p>Google emphasize on diversity, which is a important character of innovation. As we all know, diversity can bring the crash between ideas, also </p></li><li><p>related to task motivation.</p><p>because of the work in Google is challenging, employees can be stimulated their task motivation so that they have external or internal motivation to finish the task, and learn domain-relevant skills.</p></li><li><p>people core – human resource</p></li><li><p>human resource, make employees secure.</p></li></ol><h2 id="Teams"><a href="#Teams" class="headerlink" title="Teams"></a>Teams</h2><p><img src="/img/选区_021.png" alt="选区_021"></p><ol><li>the <strong>free rider problem</strong> occurs when those who benefit from resources, goods, or services do not pay for them, which results in an underprovision of those goods or services. <a href="https://en.wikipedia.org/wiki/Free_rider_problem" target="_blank" rel="external">wiki</a></li><li>choose proper team size. Make members feel they are important and responsible for the team. Make members’ individual profit related to the outcome of assembling toys. Punish free-riding. </li><li>still work</li><li>if the team is composed of very heterogeneous members with heterogeneous tasks, it is more difficult to observe and judge individual effort. </li><li>coordination effort, transaction costs…etc</li></ol><h2 id="Interactive-Value-Creation"><a href="#Interactive-Value-Creation" class="headerlink" title="Interactive Value Creation"></a>Interactive Value Creation</h2><p><img src="/img/选区_022.png" alt="选区_022"></p><ol><li><ul><li>solution information, to figure out if costumers are satisfying to the product.</li><li>need information, to figure out what needs do costuemrs have.</li></ul></li><li><p>sticky information: a problem when access need information. Access to   customers often difficult, needs are often hard to articulate.</p><p> Solvement: User integration, includes lead user methods, observe users in communities, netnography, ideation contests, user toolkits for innovation.</p></li><li><p>see HSI review1</p></li></ol><h2 id="Leadership"><a href="#Leadership" class="headerlink" title="Leadership"></a>Leadership</h2><p><img src="/img/选区_023.png" alt="选区_023"></p><ol><li><ul><li><p>transformational is a kind of leadership that is exchange-oriented. It focus on increasing the efficiency of established routines and procedures, more concerned with existing rules than making changes. Its factors are:</p><p>laissez-Faire, passive management by exception, active management by exception, contingent reward.</p></li><li><p>transformational leadership is a kind of leadership that leaders inspire, encourage employees to pursue higher performance and satisfaction. Its factors are:</p><p>idealized influence in behavior and in attribute, inspirational motivation, intellectual stimulation, individualized consideration</p></li></ul></li><li><ul><li>has benefits on influences and flexibility in brainstorming groups</li><li>improve employees’ work engagement</li><li>positively predicted organizaitional-level innovation</li><li>encourage, make workers secure and satisfied</li></ul></li></ol><h2 id="Human-Resource-Mgmt"><a href="#Human-Resource-Mgmt" class="headerlink" title="Human Resource Mgmt"></a>Human Resource Mgmt</h2><p><img src="/img/选区_024.png" alt="选区_024"></p><p>see last passage.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;some sample questions from spoiler.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="BusinessCourse" scheme="http://yoursite.com/tags/BusinessCourse/"/>
    
  </entry>
  
  <entry>
    <title>Review of HSI</title>
    <link href="http://yoursite.com/2017/02/12/HSIreview/"/>
    <id>http://yoursite.com/2017/02/12/HSIreview/</id>
    <published>2017-02-12T07:50:12.000Z</published>
    <updated>2017-02-14T16:58:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>A review of IE elective course – Human Side of Innovation</p><a id="more"></a><h2 id="exam-spoiler"><a href="#exam-spoiler" class="headerlink" title="exam spoiler"></a>exam spoiler</h2><h3 id="general-questions"><a href="#general-questions" class="headerlink" title="general questions"></a>general questions</h3><ul><li>2 longer questions and some smaller ones</li><li>60 min</li><li>most open questions, some “name/expain” questions</li><li>no math</li><li>know what papers talk about, the problems try to solve, conclusion</li></ul><h3 id="sample-questions"><a href="#sample-questions" class="headerlink" title="sample questions"></a>sample questions</h3><h4 id="Creativity"><a href="#Creativity" class="headerlink" title="Creativity"></a>Creativity</h4><p>use theories in class to explain how to foster creativity. </p><p><em>creativity: The production of novel and useful ideas in any domain. It must be different from what has been done before.</em></p><p><em>innovation: Innovation is the successful implementation of creative ideas.</em></p><p>innovation是creativity的实现</p><ul><li><p><strong>Amabile’s Model ???</strong></p><ol><li><p>task motivation stimulates your task motivation so that you have external or internal motivation to finish the task, also task motivation can make you learning domain-relevant skills.</p></li><li><p>Then you need to prepare to solve the task, in this process you can search and store relevant information for preparation via your experience and domain knowledge.</p></li><li><p>Next, search memory and generate response possibility according to the situation, this step also requires task motivation and creativity-relevant skills because it needs creative, innovative solutions for the current problem.</p></li><li><p>use domain-relevant skills to validate if your response is correct or effective.</p></li><li><p>modify, iterate according to the outcome and feedback.</p><p>提出问题，搜集背景资料，提出假设，验证假设，迭代反馈</p></li></ol></li></ul><p><img src="/img/005.png" alt="amabile&#39;s model"></p><ul><li>3 components of Creativity</li></ul><ol><li><p>task motivation (intrinsic and extrinsic)</p></li><li><p>domain relevant skills (knowledge, skills in a specific field)</p></li><li><p>creativity relevant skills (knowledge of techniques to produce creative ideas)</p><p><em>how to increase these three components?</em></p><p>内在兴趣驱动和外部因素（压力）驱动，使用相关领域知识，结合创新方法进行creativity和innovation</p><p>​</p></li></ol><table><thead><tr><th>motivation influences</th><th>positive social-environmental influences</th></tr></thead><tbody><tr><td>intrinsic</td><td>free and autonomy, optimal challenge, task match interests</td></tr><tr><td>extrinsic</td><td>competence, strategic direction, sufficient resources</td></tr></tbody></table><p>​       </p><table><thead><tr><th>skills</th><th>ofganizational support</th></tr></thead><tbody><tr><td>domain-relevant skills</td><td>provide relevant trainning, support for critical factors, task match skills</td></tr><tr><td>creativity-relevant skills</td><td>encourage unconventional thinking, support alternative solutions, time to fully capture problems, teach creativity techniques</td></tr></tbody></table><ul><li><p>threats of brainstorming (explain) <em>slides P46</em></p><p>Brainstorming: ideas can be wild, out-of-box, one time one talking, no judgements, stay focus, visual, as many as possible, be realistic finally</p><p><em>three main problems: inefficient, dominant people, too unstructured.</em></p><ol><li><p>production blocking   </p><ul><li>flow of thought may be interrupted</li><li>while others speaking, one idea may be blocked and forgotten because one talking one time</li></ul></li><li><p>social loafing</p><ul><li>participants may not work as hard as they would alone</li></ul></li><li><p>evaluation anxiety and conformity</p><ul><li>fear receive negative evaluation</li><li>fear to be diverse, stick similar ideas</li></ul></li><li><p>downward norm setting(不是很明白这个点┑(￣Д ￣)┍)</p><ul><li><p>performance across group members often converges</p></li><li><p>brainstorming participants tend to match their performance to the least productive members</p><p>$solutions$ : electronic brainstorming （网上写然后每个人的观点都po）</p><p>​             Nominal group technique(IGI，每个人独立写下自己的idea，小组讨论，个人单独评估)</p><p>​             Diversification of the team (membership change)</p><p>​                     Trained facilitators (不能重复别人的观点)</p></li></ul></li></ol></li><li><p>job satisfaction/dissatisfaction affect innovative (integreted or minicase)（<em>sides P63</em>)</p><p>job satisfaction linked to positive performance</p><p>However, discontentment can be a trigger for action if a company needs change</p><p>job dissatisfaction will have strongest positive affect for creativity when:</p><ul><li><p>when continuance commitment is high and coworkers provide useful feedback.</p></li><li><p>continuance commitment and coworker helping and support are both high.</p></li><li><p>continuance commitment and perceived organizational support for creativity are bot high.</p><p>快速响应改进，同事、组织帮助支持</p></li></ul></li><li><p>影响creativity的因素（不知道要不要考，剧透里没提到O__O “…）</p><ul><li>employee motivations has 3 types of goal orientations:<ol><li>learning orientation（想学新知识和技能）</li><li>performance prove orientation (想展示自己比别人表现得更好)</li><li>performance avoidance orientation (担心效率低下不能很好完成任务)</li></ol></li><li>bureaucracy factors<ol><li>centralization (层级化明显)</li><li>formalization (rules和standardized procedures)</li></ol></li></ul><p>centralization低时，learning orientation 促进creativity，performance avoidance orientation对creativity没那么大的负面影响</p><p>formalization低时，performance prove orientation促进creativity, performance avoidance orientation消极影响更明显</p><p>​</p></li></ul><h4 id="Teams"><a href="#Teams" class="headerlink" title="Teams"></a>Teams</h4><p>definition of Team: a social system of two or more people, in an organization, share common identity, collaborate on comon task.</p><ul><li><p>know and explain why three aspects influence decision to free ride and how to minimize it</p><ul><li>larger team size，higher incentives to free-ride</li><li>more homogeneous, lower incentives to free-ride</li><li>relations within the team</li><li>observability of effort</li><li>punishments</li><li>norms</li></ul></li><li><p>answer certain number of chances and risks about teamwork for organisation and employees</p><p>chances:</p><p><img src="/img/009.png" alt="选区_009"></p><p>risks:</p><p><img src="/img/010.png" alt="选区_010"></p></li><li><p>incentives for teams (没在剧透里提到)</p><ul><li>explicit: team bonus, profit sharing, stocks</li><li>implicit: norms (to improve team coordination and spirit, but how to establish suitable norms, supervision, exemplifying and adaptation of norms could be problem)</li><li>explicit incentives are not detrimental to creativity, incentives need to fit to the type of task</li><li>output is highest when the “impact” of an idea is rewarded</li><li>the amount of exploration is higher if participants are rewarded for their impact</li></ul></li><li><p>common problem with teamwork, why happen and how to overcome</p><ul><li><p>Free-Riding problem (有人划水= =，may cause motivation decrease, team members reduce effort)</p><p><strong>Individual effort in the team is inefficiently low(???)</strong></p><ol><li>each team member bears the marginal costs of his effort individually, marginal benefits are divided among N team members, thus the marginal benefit resulting from individual effort is lower than team’s marginal benefit from individual effort.</li><li>persons who are just interested in their own well being will thus work less hard than optimal</li></ol></li><li><p>coordination effort (transaction costs)</p></li></ul></li><li><p>best size for a team and why</p><p>我猜是it depends. 取决于成员专长、任务性质等等还要考虑free ride</p></li><li><p>how to integrate employees from two different companies (difficult question)</p><p>太多diversity可能不利于团队的performance，但diversity可以减轻free-ride，需要权衡</p></li><li><p>explain the model of Hackmans model (more general)</p><p><img src="/img/011.png" alt="选区_011"></p><p>​</p><p>three process criteria of effectiveness proposed by Hackman:</p><ol><li>effort applied to the group task<ul><li>group design: structure of the task, work will be perceived as meaningful leading to higher effort.</li><li>organization context: reward system, focus on group rewards and objectives, give positive consequences for excellend performance</li><li>group synergy: minimize coordination and motivation losses, create shared commitment</li></ul></li><li>amount of knowledge and skill applied to the group task<ul><li>group design: composition of the group, 个人有专业性，团队size合适，成员间求同存异</li><li>organizational context: education system， 组织帮助个人成长</li><li>group synergy: 不偏心团队成员，foster collective learning</li></ul></li><li>task-appropriate performance strategies<ul><li>group design: clear and intense norms, norms support to be adaptable</li><li>organizational context: clarity about parameters of performance situation(清楚任务要求，可用的资源和人), access to data about likely consequences of different strategies (能估计不同策略带来的结果)</li><li>group synergy: minimize slippage(减少划水?), create innovative strategic plans</li></ul></li></ol></li><li><p>TWQ（剧透没提，不知道要不要考不过感觉挺重要(#‵′)）</p><p>a comprehensive measure of the quality of cooperations in teams</p><ul><li>communication: team members communicate frequently, directly, personally, spontaneously, information shared openly, preciesely and useful (团队成员之间友好亲切直接交流不要套路)</li><li>coordination: closely harmonized subtasks, clear fully comprehended and accepted by all members (大家都同意的合理分工)</li><li>balance of member contributions: avoid imbalance of contributions, recognize individuals potential</li><li>mutual support: help each other, less conflicts, suggestions are respected, able to reach consensus (团员互相帮助尊重彼此达到共识)</li><li>effort: everyone fully pushes, puts much effort</li><li>cohesion: feel proud of the team, personal attraction among members</li></ul><p>​</p></li></ul><h4 id="Interactive-value-creation"><a href="#Interactive-value-creation" class="headerlink" title="Interactive value creation"></a>Interactive value creation</h4><p>trade-offs of cooperate or make by self</p><p><img src="/img/012.png" alt="选区_012"></p><ul><li><p>know and explain main kinds of information required for innovation mgmt (slides 125)</p><ol><li><p>solution information （解决方案是否满足需求）</p><p>how can a certain need be satisfied with a product or service?</p><p>what kind of principles, methods to solve the problem?</p><p><em>may encounter local search bias (总是用老办法是不行滴，要想新办法)</em></p></li><li><p>need information (客户有啥需求)</p><p>utility and preference of customers</p><p>what kind of problem should be solved?</p><p>this kind of information allos to put the right innovation to the market.</p><p><em>may encounter sticky problem</em></p></li></ol></li><li><p>explain the problem of sticky information for innovation mgmt (<em>slides 128</em>)</p><p>sticky information: a problem when access need information. Access to customers often difficult, needs are often hard to articulate.</p><p>Solvement: User integration.</p></li><li><p>know one of the methods of user integration and explain how this method works</p><blockquote><p>when the user is (partly) the product creator, successful products will arrive. </p></blockquote><ul><li><p>Lead user methods</p><p>see below</p></li><li><p>observe users in communities</p><ul><li><p>empathic design: 通过视奸（不是提问）客户使用产品的情况确定客户潜在需求</p><p>goal: </p><ol><li>gather information about customer habits, interaction of product with consumers environment, assessment of product features, needs not articulated yet.</li><li>achieve breakthrough, accelerate product development cycles</li><li>improve products via commercializing innovations already developed</li></ol><p>requirements:</p><ol><li>specialists and methodological knowledge</li><li>short process, longer preparation</li><li>low to medium resource demand</li><li>good opportunities for outsourcing to specialized agencies</li></ol><p>how to apply it?</p><ol><li>search field definition: define users for observation, issues should be focus on, research team</li><li>capturing data</li><li>evaluation and documentation</li><li>reflection and analysis</li></ol><p>benefits:</p><ol><li><p>gathering a differentiated picture of the needs of customers</p></li><li><p>determine latent needs of customers</p></li><li><p>rather low cost, low risk</p></li><li><p>create opportunities for differentiation strategies</p><p><em>cannot replace market research</em></p></li></ol></li><li><p>netnography</p><p>what is netnography: a qualitative research approach to analyze the customer and user dialogues in existing online communities(视奸用户的评论= =)</p><p>goal:</p><ol><li>typologies of user groups</li><li>identification of user innovations, user generated contents, product prototypes</li><li>identification of opinion leaders, lead users</li></ol><p>requirements:</p><ol><li>process community input</li><li>user input data</li><li>find relevant user input</li><li>specilized agencies’ support</li></ol><p>how to apply it?</p><ol><li>search field definition: systemize topics, trends, markets, products</li><li>identificate and select online communities and source</li><li>community observation, data gathering</li><li>qualitative in depth analysis of consumer insights</li><li>insight translation into product solutions</li></ol><p>benefits:</p><ol><li>gain unbiased consumer insights without informing competitors</li><li>classify and posit products within a perceptual map</li></ol><p>cons:</p><ol><li>solutions can be biased by selection of inputs</li><li>companies often lack the capability to communicate with communities</li><li>hard to evaluate large quantities of data</li><li>hard to test your findings</li></ol></li></ul></li><li><p>ideation contests</p></li><li><p>user toolkits for innovation</p></li></ul><p>​</p></li><li><p>know and explain lead user method, how it works</p><p>what is lead user methods?</p><ul><li>idea: collaborate with these lead users to generate innovative concepts with functional novel elements, use specific search techniques to identify lead users, work with lead users in concept generation workshops to codevelopt. </li><li>goal: innovative insights into technical solutions, generating trend explorations, identification of product requirements from user perspective, identification of innovative ideas to diversify business activities.</li><li>requirements: high resource effort, senior developers of products also need to participate, dedicated training in lead user identification.</li></ul><p>how to apply lead user methods?</p><ul><li>define project team and project objectives</li><li>analyze trends and needs in search fields</li><li>identificate eligible lead users and encourage them to participate</li><li>foster concept generation, evaluate and refine ideas</li></ul><p>benefits of using Lead user method (how it works)</p><ul><li>identification of strong market opportunities</li><li>concepts are developed with direct input from “lead users”</li><li>get new products and services faster to the market</li></ul></li></ul><ul><li><p>benefits and disadvantages of user integration (but in specific situation)</p></li><li><p>two problems of innovation mgmt </p><p>slides P189有考试剧透</p></li></ul><h4 id="Leadership"><a href="#Leadership" class="headerlink" title="Leadership"></a>Leadership</h4><blockquote><p>The ability to influence, motivate, and enable others to contribute to the effectiveness and success of the organization.</p><p>management is coping with complexity, while leadership is coping with change</p></blockquote><ul><li><p>name and describe two transformational and transactional leadership factors</p><ul><li>transactional leadership (managerial leadereship)<ul><li>exchange-oriented leadership (奖惩分明)</li><li>focus on increasing the efficiency of established routines and procedures, more concerned with following existing rules than with making changes to the structure of the organization. (强调规则纪律)</li><li>factors:<ol><li>laissez-Faire: 缺少领导力，不干预，放养</li><li>passive management-by-exception: 出了问题才处理</li><li>active management-by-exception: keep tracks of all mistakes, monitor subordinates</li><li>contingent reward: set performance goals, clarify expectations, give recognition upon goal attainment.</li></ol></li></ul></li><li>transformational leadership<ul><li>visionary, inspirational and stimulating leadership engender higher performance and satisfaction of employees (鼓励型领导)</li><li>share vision of future, support subordinates, recognize individual difference, sets high expectations</li><li>factors:<ol><li>idealised influence (behavioural): communicate mission and values，跟员工讲他们的重要价值，让员工觉得自己很重要</li><li>idealised influence (attributed): perceived as exceptional, confident and trustworthy, 让员工以为老板打工为傲</li><li>inspirational motivation: articulate visions and share goals optimiscally and enthusiastically </li><li>intellectual stimulation: encourage others to develop new approaches, 鼓励从多角度看问题</li><li>individualised consideration: provide coaching, treat subordinates on a one-to-one basis, 帮助员工成长</li></ol></li></ul></li></ul></li><li><p>why transformational leadership is associated positively with innovation</p><p>从transformational leadership的factors回答应该就可以吧我猜。</p><p>有几个文章比较两种方式哪个好，结果是</p><ol><li>transformational leadership有利于fluence and flexibility in brainstorming groups</li><li>在financial service organizations中，transformaitonal leadership更支持创新，员工表现也更好，</li><li>Transformational leadership positively predicted organizational-level innovation through enhanced support for innovation</li><li>提升员工的work engagement</li></ol></li><li><p>the difference between transformational leadership and transactional leadership, where to apply appropriately</p><p>感觉还是根据上边的factors答</p></li><li><p>authentic leadership (剧透没提，slides里有)</p><p>authentic leadership positively associated with creativity, job satisfaction, voluntary extra effort, performance and sales</p><p>featues:</p><ul><li>self-awareness: 知道员工的长短处</li><li>relational transparency: 没有套路展示自我想法</li><li>balanced processing: 在决策前悉心听取每个意见</li><li>moral perspective: 用belief驱动action，而不是外部压力</li></ul></li></ul><h4 id="Human-Resources-in-Innovation-Management"><a href="#Human-Resources-in-Innovation-Management" class="headerlink" title="Human Resources in Innovation Management"></a>Human Resources in Innovation Management</h4><blockquote><p>get the right people, and get them together, incentivize them correctly</p></blockquote><ul><li><p>how opponents can be both beneficial and detrimentla to innovation progress (difficult)</p><p>​</p></li></ul><ul><li>name two barriers to innovation and what kind of promotors to overocome them</li></ul><p>  to overcome these two barriers:</p><ul><li><p>power promotors (promotors by hierarchical power)</p><p>surmount barriers of will through their hierarchical potential</p></li><li><p>promotors  (promoters by know how)</p><p>surmount barriers of capability through their expert knowledge</p></li></ul><ul><li><strong>principal agent theory</strong>: main assumptions, where to apply, two problems they trigger, how to mitigate the problem</li></ul><blockquote><p>PAT deals with relations and contracts, in particular in situations with asymmetric information.</p></blockquote><p>  PAT的概念看看就好。。感觉并不会直接问概念</p><ul><li><p>two problems:</p><ul><li>certain characteristics of the agent not are not known to the principal (Hidden Information)</li></ul><p>Is it possible to design contracts that makes only “suitable” candidates want to sign, and for agents to submit/reveal their characteristics?</p><p>agents可能有一些principal不知道的状态或信息，这些是外源性(exogenous)的</p><ul><li>agents actions are hard to monitor (hidden action)</li></ul><p>what actions should be taken to monitor the agent? and is it possible to design contracts make agents choose “right” actions?</p><p>agnets有不会被principal观察到的行动，这些信息缺失是内源性(endogenous)的</p></li><li><p>how to reduce uncertainty?</p><ul><li>signalling: a mechanism where agent credibly conveys information about itself to the principle</li><li>screening (opposite of signalling): reveal true characteristics of the agent, principles offer a menu of contracts in order to separate the different types, that is design the contract that only suitable candidates accept it.</li></ul></li><li><p>Hidden action problem:</p><p> Agent’s action are not observable, but their actions influence the outcome of the principal. Principal and agents have different interests, <em>how can the principal induce the agents to choose their actions in the principal’s interest?</em></p><ul><li><p>reduce informaiton assymmetry (monitoring)</p><p>principal wants to control agent’s actions via monitoring, it will lead to direct costs (like cameras, supervisors) and indirect costs (like trust issues, reduce creativity)</p></li></ul></li></ul><pre><code>- alignmetn of interests  1. performance based payments     may face constraints that need to provide sufficient incentives and proper kind of incentives.     may face *Moral Hazard*: 参与合同的一方所面临的对方可能改变行为而损害到本方利益的风险.(缺少不违约的激励)     if performance based payments doesn&apos;t work, you should make employees feel like a part of the organization, ensure the security of workers and more effort is not a downside, make them believe company success is individuals&apos; success  2. change working conditions</code></pre><ul><li><p>main difference between human capital and signalling theory, explain them, describe singalling theory in general, where need to apply it   <em>(slides P218 – 223 用学历和生产力的关系举例)</em></p><ul><li><p>human capital theory explains differences due to different stock in human capital(knowledge, skills…), namely education increases productivity</p></li><li><p>signaling theory provides an alternative expalnation which maintains education certificates reveal productivity.</p><p>看的不是很明白。。感觉好像一个是看即时成果一个是看潜力</p></li></ul></li><li><p>roles in innovation process</p><ul><li><p>the “champion”: someone who pushes an innovation.</p><p>direct motivational influences:</p><ul><li>expresses enthusiasm and confidence</li><li>persists under adversity</li><li>gets the right people involved</li></ul></li></ul></li></ul><ul><li><p>promoter roles</p><p>barriers, power-bases and contributions define a promotor role</p><ul><li>persons which actively and intensively support an innovation</li><li>start an innovation process</li><li>sustain a high activity level</li><li>terminate the decision process</li></ul></li><li><p>other classification of promotors:</p><ol><li><p>process promotor: 选人、联系sponsors和experts、跟进流程、领导创新团队</p></li><li><p>relationship promotor：social, external networks, find external parterners, plans, build trust</p></li><li><p>technological gatekeeper: expert knowledge</p></li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A review of IE elective course – Human Side of Innovation&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="BusinessCourse" scheme="http://yoursite.com/tags/BusinessCourse/"/>
    
  </entry>
  
  <entry>
    <title>coprocessing and GPU</title>
    <link href="http://yoursite.com/2017/02/04/coprocessing/"/>
    <id>http://yoursite.com/2017/02/04/coprocessing/</id>
    <published>2017-02-04T07:50:12.000Z</published>
    <updated>2017-02-13T18:53:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>This chapter talks about gpu and coprocessing.<br><a id="more"></a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Modern Porcessors has performance limitations, because the number of transistors on a chip keeps growing quickly while the power density of transistors was constant.</p><p><em>constant chip size and increasing number of transistors increases overall power consumption and the produced heat.</em></p><p>Thus, cooling and energy consumption became two limits on the amount of power a processor can use. <strong>Modern processors are limited by a fixed energy budget</strong></p><p>To remain in the power constraints, modern proceessors can operate with lower clock rate or turn of parts of the chip, but how to use increasing number of constraints?</p><p><strong>Future machines are expected to consist of a set of heterogeneous processors, each processor is optimized for a certain application scenario.</strong></p><p>For database, we can use these processors to accelerate query processing.</p><h1 id="GPU-architecture"><a href="#GPU-architecture" class="headerlink" title="GPU architecture"></a>GPU architecture</h1><p>For GPU, data copying host, invocate computer kernels and kernels run asynchronously.</p><h2 id="Threads-on-GPU"><a href="#Threads-on-GPU" class="headerlink" title="Threads on GPU"></a>Threads on GPU</h2><p>keep things simple</p><ul><li>don’t try to reduce latency, but hide it</li><li>assume data parallelism and restrict synchronization</li><li>hardware thread scheduling</li></ul><h2 id="CPUs-vs-GPUs"><a href="#CPUs-vs-GPUs" class="headerlink" title="CPUs vs. GPUs"></a>CPUs vs. GPUs</h2><p>CPU: task parallelism</p><ul><li>relatively heavyweight threads</li><li>each thread managed explicitly</li><li>threads run different code</li></ul><p>GPU: data parallelism</p><ul><li>lightweight threads</li><li>threads scheduled in batches</li><li>all threads run same code</li></ul><h1 id="Relational-Co-processing-on-GPUs"><a href="#Relational-Co-processing-on-GPUs" class="headerlink" title="Relational Co-processing on GPUs"></a>Relational Co-processing on GPUs</h1><h2 id="Selection"><a href="#Selection" class="headerlink" title="Selection"></a>Selection</h2><p>choose a subset of tuples from a relation R satisfying a predicate and discard the rest.</p><p>How to parallelize selections efficiently?</p><ul><li>concurrent may writes corrupt data structures</li><li>latching may serialize threads and nullify the preformance</li></ul><p><strong>Key idea: Pre-compute write locations</strong></p><ol><li><p>Prefix Scans: important building block for parallel programs, given an input array $R<em>{in}, R</em>{out}$ is computed as: $R<em>{out}[i] = R</em>{in}[0] + … + R<em>{in}[i-1] (1 \le i&lt;|R</em>{in}|)$, $R_{out}[0] = 0$ <img src="/img/prefix_sum.png" alt="prefix_sum"></p></li><li><p>Parallel Filter: create an array flags of the same size as R and init with zeros</p></li></ol><p><img src="/img/001.png" alt="选区_001"></p><h1 id="Query-Processing-and-Data-Transfer-Bottleneck"><a href="#Query-Processing-and-Data-Transfer-Bottleneck" class="headerlink" title="Query Processing and Data Transfer Bottleneck"></a>Query Processing and Data Transfer Bottleneck</h1><p>Co-processor’s memory capacity quite small, cannot fit all data on it.</p><p>Cache input data in co-processor memory and process data locally as much as possible.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This chapter talks about gpu and coprocessing.&lt;br&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="DatabaseTechnology" scheme="http://yoursite.com/tags/DatabaseTechnology/"/>
    
  </entry>
  
  <entry>
    <title>DBT 07 Concurrency Control</title>
    <link href="http://yoursite.com/2017/02/02/concurrency/"/>
    <id>http://yoursite.com/2017/02/02/concurrency/</id>
    <published>2017-02-02T06:49:04.000Z</published>
    <updated>2017-02-04T12:53:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>This chapter talks about how to use concurrency control to implement isolation and consistency of database, include concurrency execution, serializability theory, 2 phase locking, hierarchical locking, how to deal with deadlocks.</p><a id="more"></a><h2 id="several-definitions"><a href="#several-definitions" class="headerlink" title="several definitions:"></a>several definitions:</h2><ol><li><p>transaction: a unit of work, has multiple data accesses and updates</p><p> transaction introduces two problems:</p><ul><li><p>what to do when system has failure? == Recovery</p></li><li><p>what happens when two transactions try to access the same object? == <strong>Concurrency</strong></p></li></ul></li></ol><ol><li><p>ACID properties</p><ul><li><p>Atomicity: operations only have two states, namely complete or none of them complete</p></li><li><p>Consistency: transactions applied to a consistent database and produce consistent database(preserve integrity constraints)</p></li><li><p>Isolation: A transacion executes as it is the only transaction running in the system</p></li><li><p>Durability: The effects of committed transactions are reflected in the database even after failures</p></li></ul></li><li><p>Transaction Model<br>Assuem we have database items X, Y, Z, …</p><ul><li><p>Ri(X) – Transaction Ti reads item X</p></li><li><p>Wi(X) – Transaction Wi writes item X</p></li><li><p>Ai – Transaction Ti aborts</p></li><li><p>Ci – Transaction Ti commits</p></li></ul></li></ol><p><em>Concurrency: implement isolation and consistency</em></p><h2 id="Concurrency-Execution-–-interleaved-operations"><a href="#Concurrency-Execution-–-interleaved-operations" class="headerlink" title="Concurrency Execution – interleaved operations"></a>Concurrency Execution – interleaved operations</h2><p>Why use concurrency?</p><ul><li><p>use resources efficiently</p></li><li><p>keep fairness of database</p></li><li><p>short transactions don’t need to wait long transactions finished</p></li></ul><h3 id="Serializability-theory"><a href="#Serializability-theory" class="headerlink" title="Serializability theory"></a>Serializability theory</h3><p>interleaved operations may cause inconsistency. So we need a <strong>schedual</strong> which is <em>a partial order of transaction operations that indicates how they interleaved</em>, namely an order of how operations of a transactioni executed.<br>For a correct schedual:</p><ul><li><p>all operations appear in the same order as they appear in transaction</p></li><li><p>a complete order of all conflicting operations of all Ti, Tj is sepcified. </p><p>a <strong>Serial schedual</strong> is a schedual <em>where all oeprations of each transactions appear in consecutive blocks</em></p><p>(the order of transactions can be swapped, but the order of operations in a transaction cannot be swapped)</p></li></ul><h4 id="conflict-conflict-equivalent-conflict-serializable"><a href="#conflict-conflict-equivalent-conflict-serializable" class="headerlink" title="conflict, conflict-equivalent, conflict-serializable"></a>conflict, conflict-equivalent, conflict-serializable</h4><p> for two transactions Ti, Tj:</p><table><thead><tr><th>not conflict</th><th>may conflict</th></tr></thead><tbody><tr><td>ri(X); rj(Y)</td><td>ri(X); wi(Y)</td></tr><tr><td>ri(X); Wj(Y) (X!=Y)</td><td>wi(X); wj(X)</td></tr><tr><td>wi(X);rj(Y) (X!=Y)</td><td>ri(X); wj(X)</td></tr><tr><td>wi(X);wj(Y) (X!=Y)</td><td>wi(X); rj(X)</td></tr></tbody></table><p>operations cannot be swapped when: <strong> two transactions involve same database element and at least one operation is write</strong></p><p>two schedual S and S’, if:</p><ul><li><p>they have same transactions and operations</p></li><li><p>one can be transformed to another via swap non-conflicting operators</p></li></ul><p>Then they are <strong>conflict-equivalent</strong></p><p>if and only if a schedule is conflict-equivalent to some serial schedual, it is <strong>conflict-serializable</strong></p><h4 id="precedence-Graph"><a href="#precedence-Graph" class="headerlink" title="precedence Graph"></a>precedence Graph</h4><p>can check if a schedual S is conflict-serializable</p><p>for a schedual S has transactions T1, T2, action A1 in T1, action A2 in T2, if:</p><ul><li><p>A1 ahead of A2</p></li><li><p>A1 and A2 has the same database element</p></li><li><p>at least one of A1 A2 is write</p></li></ul><p>Then, T1 <em>takes precedence of</em> T2, A1–&gt;A2<br>check the order of actions which have same database element to decide ?–&gt;?, and that’s a directed line in precedence graph.</p><p>If no cycles in precedence graph, then S is conflict-serializable.</p><h2 id="Locking"><a href="#Locking" class="headerlink" title="Locking"></a>Locking</h2><p>locking is used to prevent unserializable behavior</p><ul><li><p>li(X): Ti request a lock on X</p></li><li><p>Ui(X): Ti unlocks its lock on X</p></li></ul><p>Properties:</p><ol><li><p><em>Legality</em>: two transactions cannot lock the same element </p></li><li><p><em>Consistency</em>: Before write or read X, the transaction T needs to lock X, if there is li(X), then there must be a ui(X) after.</p></li></ol><h3 id="2-phase-locking"><a href="#2-phase-locking" class="headerlink" title="2-phase locking"></a>2-phase locking</h3><p>to guarantee that a legal schedual of consistent transactions is conflict-serializable</p><p><strong>1st phase: locks obtained only</strong></p><p><strong>2nd phase: locks relinquished only</strong></p><p>But 2-phase locking cannot prevent deadlocks, when they are waiting each other’s unlocking.</p><h3 id="several-lock-models"><a href="#several-lock-models" class="headerlink" title="several lock models"></a>several lock models</h3><ul><li><p>shared lock (can have many in one transaction) – <strong>used to protect read</strong></p></li><li><p>exclusive lock (only one in one transaction) – <strong>used to protect both read and write access</strong></p></li></ul><p>ri(X) must be preceded by sli(X) or xli(X), wi(X) must be preceded by xli(X)</p><p>requirements:</p><ol><li><p>Consistency: ri(X) after sli(X) or xli(X), and no ui(X) between them; wi(X) after xli(X), no ui(X) neither.</p></li><li><p>Two-phase locking: locking must precede unlocking.</p></li><li><p>Legality: Before unlock, we cannot give same type of lock to same database element from different transactions (such as Ti, Tj).</p></li></ol><p>If you got xli(X), no xlj(X) or slj(X)</p><p>If you got sli(X), no xlj(X).</p><p>The same database element from the same transaction can be locked by share lock and exclusive lock, but it must under the situation that no other transacitons have conflicts.</p><p>Compatability matrix – describe lock management policies</p><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>S</td><td>X</td></tr><tr><td>S</td><td>yes</td><td>no</td></tr><tr><td>X</td><td>no</td><td>no</td></tr></tbody></table><h3 id="Hierarchical-Locking"><a href="#Hierarchical-Locking" class="headerlink" title="Hierarchical Locking"></a>Hierarchical Locking</h3><p>more fine grained locking ==&gt; better concurrency</p><p>Granularities(top - down) :</p><p>Database – Table – Table partition – Index – Page – Tuple – Index page</p><p>IS: intention to obtain S lock at finer granularity</p><p>IX: intention to obtain X lock at finer granularity</p><p>SIX: S lock on this granularity, intention to obtain X at a finer granularity</p><p>but:</p><pre><code>- lock size increase- big locks force other transactions to wait</code></pre><p>use intention locks to implement hierarchical locking protocol:</p><ol><li><p>start at root</p></li><li><p>have IS or IX on all ancestors to get IS or S</p></li><li><p>To get IX, SIX or X, must have IX or SIX on all ancestors</p></li></ol><table><thead><tr><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>IS</td><td>IX</td><td>SIX</td><td>S</td><td>X</td></tr><tr><td>IS</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>N</td></tr><tr><td>IX</td><td>Y</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr><tr><td>SIX</td><td>Y</td><td>N</td><td>N</td><td>N</td><td>N</td></tr><tr><td>S</td><td>Y</td><td>N</td><td>N</td><td>Y</td><td>N</td></tr><tr><td>X</td><td>N</td><td>N</td><td>N</td><td>N</td><td>N</td></tr></tbody></table><p>The lock manager:</p><ul><li><p>acquireLock(T, X, mode)</p></li><li><p>releaseLock(T, X, mode)</p></li></ul><h2 id="Deal-with-deadlocks"><a href="#Deal-with-deadlocks" class="headerlink" title="Deal with deadlocks"></a>Deal with deadlocks</h2><p>deadlock: 2 transactions are waiting for each other’s locks to be released</p><p>2 ways: delete or avoid deadlocks</p><ul><li>Deadlock avoidance: impose order in which locks can be acquired, transactions notify they need locks in advance, if deadlock happened, abort T rather than blocking it.</li></ul><p>However: transactions usually don’t know if they need locks in advance.</p><ul><li>Deadlock prevention: ABort transactions waiting too long – timeouts</li></ul><p>However: how to pitch timeout parameter?</p><ul><li>Deadlock detection: Ti-&gt;Tj means Ti is blocked waiting for Tj to release a lock. If cycle, then deadlocks happen, roll back releases locks automatically</li></ul><p>Dreadlock algorithm???</p><h2 id="Isolation-levels"><a href="#Isolation-levels" class="headerlink" title="Isolation levels"></a>Isolation levels</h2><p>blocking impair the response time of transactions.</p><p>Some applications can tolerate a bit of “dirtyness”, they don’t always need full serializability.</p><p>=&gt; solution: defining levels of isolation to trade isolation and concurrency.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This chapter talks about how to use concurrency control to implement isolation and consistency of database, include concurrency execution, serializability theory, 2 phase locking, hierarchical locking, how to deal with deadlocks.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="DatabaseTechnology" scheme="http://yoursite.com/tags/DatabaseTechnology/"/>
    
  </entry>
  
  <entry>
    <title>DBT 08 Recovery from system failures</title>
    <link href="http://yoursite.com/2017/01/30/Recovery/"/>
    <id>http://yoursite.com/2017/01/30/Recovery/</id>
    <published>2017-01-30T21:33:06.000Z</published>
    <updated>2017-02-15T08:37:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>This chapter talks about how to do recovery when database system has crashes. To solve the atomicity and durability, we use logging. It mainly has Undo logging, Redo logging, and Undo/Redo logging. Undo logging records history value, commit after all changes write to the disk, ignore complete transactions; Redo logging records future value, commit before write to disk, ignore incomplete transactions and repeat complete transactions. We also use checkpoint in recovery only read certain transactions, also it allows other active transactions run when checkpointing.</p><a id="more"></a><h2 id="Failure-modes"><a href="#Failure-modes" class="headerlink" title="Failure modes"></a>Failure modes</h2><h3 id="Error-type"><a href="#Error-type" class="headerlink" title="Error type:"></a>Error type:</h3><ul><li><p>transaction failure: </p><p>  abort, error in application (like division<br>  by zero)</p></li><li><p>system failure: </p><p>  crash in DBMS, OS, hardware<br>  data in main memory is lost</p></li><li><p>media failure: </p><p>  head crash, catastrophy, data destroyed…</p></li></ul><p>==&gt; <strong>solution: logging</strong> record what have done to the database, <em>solve atomicity and durability</em></p><p>logging has three attributes, namely:</p><ul><li>rules</li><li>recovery methods</li><li>checkpoint</li></ul><h3 id="transactions-processing"><a href="#transactions-processing" class="headerlink" title="transactions processing"></a>transactions processing</h3><p>buffer manager, log manager</p><ol><li><p>transaction requests a database element from bufffer manager</p></li><li><p>buffermanager reqrieves from disk if needed</p></li><li><p>fetch element into local address space</p></li><li><p>modify or new element inside the address space</p></li><li><p>transaction returns new element to buffer manager</p></li><li><p>buffer manager writes element back to disk</p></li></ol><h3 id="primitive-operations"><a href="#primitive-operations" class="headerlink" title="primitive operations"></a>primitive operations</h3><ul><li><p>INPUT(X): copy X from disk into buffer</p></li><li><p>READ(X, t): Copy X from buffer into transaction address space</p></li><li><p>WRITE(X, t): copy value produced by t to element X</p></li><li><p>OUTPUT(X): copy containing X from buffer to disk</p></li></ul><h2 id="undo-logging"><a href="#undo-logging" class="headerlink" title="undo logging"></a>undo logging</h2><h3 id="log-records"><a href="#log-records" class="headerlink" title="log records:"></a>log records:</h3><ul><li><p>&lt; START T &gt;: start transaction T</p></li><li><p>&lt; T,X,v &gt;: the value v of database element X was changed by transaction T. (v is the history value)</p></li><li><p>&lt; COMMIT T &gt;: T complete, no further change</p></li><li><p>&lt; ABORT T &gt;: T has no effect on the disc</p></li><li><p>FLUSH LOG: ask buffer manager to write all log from blocks to disc</p></li></ul><p><strong><em>undo logging</em>: record past value v of &lt; T,X,v &gt;, recover to the history value, read log from bottle to the bottom until find the place where crashes happen.</strong></p><h3 id="Rules"><a href="#Rules" class="headerlink" title="Rules:"></a>Rules:</h3><ol><li><p><em>&lt; COMMIT T &gt; after OUTPUT(X) to the disc</em></p></li><li><p><em>record &lt; T,X,v &gt; after changes done, before write new changes to the disc</em></p></li></ol><h3 id="Recovery"><a href="#Recovery" class="headerlink" title="Recovery:"></a>Recovery:</h3><p>If crashes happen when:</p><ol><li><p>after FLUSH LOG —- do nothing because &lt; COMMIT T &gt; has been executed and changes in the disc</p></li><li><p>after &lt; COMMIT T&gt; before FLUSH LOG —- recovery to history value and write &lt; ABORT T &gt; to log then FLUSH LOG</p></li><li><p>write changes into disc —- take T as uncommitted, do same as above</p></li><li><p>record &lt; T,X,v &gt; —- do same as above </p></li></ol><h3 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h3><p>BUT: we can find that use undo logging we need to <strong>read all logs</strong>, it cost a lot</p><p>=&gt; So we use <strong>checkpoint</strong>, then we need to just recovery until we read &lt; CKPT &gt;</p><p>BUT: database is blocked during the checkpoint, no new transactions will be accepted</p><p>=&gt; checkpoint only for certain transactions, use &lt; START CKPT (T1, T2, … Tn) &gt; and <end ckpt=""></end></p><p>reading from back to first, if we meet:</p><ul><li><p>first &lt; END CKPT &gt; : recovery only till next &lt; START CKPT &gt;</p></li><li><p>first &lt; START CKPT T1, …Tk &gt;, means we have system failure during checkpointing, T1 … Tk are the single active transaction at this time</p></li></ul><h2 id="redo-logging"><a href="#redo-logging" class="headerlink" title="redo logging"></a>redo logging</h2><p><strong>record future value of an element, redo the changes</strong></p><h3 id="Rule"><a href="#Rule" class="headerlink" title="Rule:"></a>Rule:</h3><ol><li><p><em>use before write changes to disc</em></p></li><li><p><em>all logs of T and &lt; COMMIT T &gt; must be wrriten to disc</em></p></li></ol><h3 id="redo-recovery-process"><a href="#redo-recovery-process" class="headerlink" title="redo recovery process"></a>redo recovery process</h3><p>if crashes happen when:</p><ol><li><p>write to disc, namely after FLUSH LOG: T will be regarded as committed, so redo the changes as the redo log state (maybe redundant)</p></li><li><p>At &lt; COMMIT T &gt; : if &lt; COMMIT T &gt; write to the disc, same above, else same below</p></li><li><p>before &lt; COMMIT T &gt;: T is not complete, do nothing and &lt; ABORT T &gt;</p></li></ol><p>BUT: Although T is committed, it is possible that the change is not in the disc, namely crashes happen when OUTPUT.</p><p>=&gt; write all database elements on disc during checkpointing that have been changed by committed but incomplete transactions, don’t need to wait all active transactions finished.</p><h3 id="checkpoint-recovery"><a href="#checkpoint-recovery" class="headerlink" title="checkpoint recovery"></a>checkpoint recovery</h3><p>at &lt; END CKPT &gt;: All transactions committed before &lt; START CKPT (T1, T2,…, Tn) &gt; are on disc, but all T1 … Tk started after START are unsafe</p><p>at &lt; START CKPT … &gt;: crashes occurred during checkpointing, search next &lt; END CKPT &gt;(backward), then continue to next &lt; START CKPT … &gt;, redo all transactions.</p><p>redo after commit, if a transaction not commit, abort it.</p><h2 id="undo-vs-redo"><a href="#undo-vs-redo" class="headerlink" title="undo vs. redo"></a>undo vs. redo</h2><ul><li><p>undo logging: allowed to commit when all changes are on disk, write COMMIT to log after all values are on disk, log holds old values.<br><strong>Incomplete transactions are rolled back, complete transactions are ignored.</strong> </p></li><li><p>redo logging: allowed to commit when changes are on log, COMMIT is written to log before any value is written to disk, log holds new values.<br><strong>Incomplete transactions are ignored, complete transactions are repeated.</strong></p></li></ul><p>drawbacks: </p><ul><li><p>undo log: Data must be written immediately after the end of a transaction – too many I/O</p></li><li><p>redo log: All data stay in the buffer till COMMIT – high requirements of memory</p></li></ul><h2 id="undo-redo-logging"><a href="#undo-redo-logging" class="headerlink" title="undo/redo logging"></a>undo/redo logging</h2><p>flexible, but need to record more info in log.</p><p>&lt; T,X,v,w &gt;: (v is the old value, w is the new value)</p><p>Rule: Update log &lt; T,X,v,w &gt; must have been written to disc.</p><p>Before X changed by T has written on disc, &lt; COMMIT T &gt; can be written on disc before/after OUTPUT.</p><h3 id="Recovery-1"><a href="#Recovery-1" class="headerlink" title="Recovery"></a>Recovery</h3><p>Redo all committed transactions in time order.<br>Undo all uncommitted transactions in reverse time order.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>For Undo logging, do nothing to transactions committed, do undone to transactions uncommitted.<br>FOr Redo logging, do nothing to transactions uncommitted, do redone to transactions committed.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This chapter talks about how to do recovery when database system has crashes. To solve the atomicity and durability, we use logging. It mainly has Undo logging, Redo logging, and Undo/Redo logging. Undo logging records history value, commit after all changes write to the disk, ignore complete transactions; Redo logging records future value, commit before write to disk, ignore incomplete transactions and repeat complete transactions. We also use checkpoint in recovery only read certain transactions, also it allows other active transactions run when checkpointing.&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="http://yoursite.com/categories/notes/"/>
    
    
      <category term="TUBcourse" scheme="http://yoursite.com/tags/TUBcourse/"/>
    
      <category term="DatabaseTechnology" scheme="http://yoursite.com/tags/DatabaseTechnology/"/>
    
  </entry>
  
</feed>
